{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.VGG模型（2014年）\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 理解为什么可以用3x3的卷积核来代替更大尺寸的卷积核\n",
    "\n",
    "2. 会计算CNN网络的参数量\n",
    "\n",
    "3. 熟悉模型参数的初始化方法\n",
    "\n",
    "4. 会调节超参数：周期（epoch）、优化器的学习率（LR）、mini-batch\n",
    "\n",
    "5. 理解批量归一化（BN）对训练深层网络的重要性\n",
    "\n",
    "6. 会使用torch.utils.data.random_split划分数据集\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 VGGNet\n",
    "\n",
    "VGG网络，全称为Visual Geometry Group网络，是一种在深度学习领域具有重要影响力的卷积神经网络（CNN）架构。它由牛津大学的视觉几何组（Visual Geometry Group）提出，并在2014年的ImageNet挑战赛中取得了优异的成绩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 VGG的关键技术\n",
    "\n",
    "与AlexNet相比，VGG（Visual Geometry Group）网络的改进和优化：\n",
    "\n",
    "（1）统一的3×3卷积核\n",
    "VGGNet在所有卷积层中统一使用了3x3的卷积核，而AlexNet使用了不同大小的卷积核（如11x11、5x5等）。统一的3×3卷积核，能够在保证感受野大小的同时，减少模型的参数量。同时，由于3×3卷积核可以看作是一种特殊的1×1和5×5卷积核的组合，因此它能够在一定程度上模拟更大卷积核的效果，提高模型的表达能力。\n",
    "\n",
    "（2）深度与性能的关系\n",
    "VGGNet通过构建不同深度的网络结构，探索了卷积神经网络的深度与其性能之间的关系。实验结果表明，随着网络深度的增加，模型的性能也会相应提高。这为后续的研究提供了重要的启示，即构建更深的网络结构是提高模型性能的有效手段之一。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 VGG16模型\n",
    "\n",
    "<img src=\"./images/VGG16.png\" style=\"zoom:80%;\" />\n",
    "\n",
    "VGG16模型中每部分的参数量计算方法：\n",
    "\n",
    "1. **卷积层参数**：每个卷积层的参数量由以下公式给出： 参数量=卷积核尺寸×输入通道数×输出通道数 \n",
    "\n",
    "对于VGG16，所有的卷积层使用3x3的卷积核，并且卷积层的输出通道数分别为64, 128, 256, 512（每个数量级重复三次），除了最后一组卷积层，它们是512, 512, 512。输入通道数从1开始（对于第一个卷积层，因为输入图像是单通道的灰度图），然后是前一层的输出通道数。\n",
    "\n",
    "2. **池化层参数**：池化层没有参数，因此它们的参数量为0。\n",
    "\n",
    "3. **全连接层参数**：每个全连接层的参数量由以下公式给出： 参数量=(前一层节点数+1)×本层节点数参数量=(前一层节点数+1)×本层节点数 \n",
    "   其中“+1”是因为全连接层包含偏置项。VGG16的全连接层的节点数分别为4096, 4096, 和类别数（例如1000）。\n",
    "\n",
    "4. **偏置参数**：每个有偏置的层都有一个额外的偏置参数，其数量等于该层的输出通道数或节点数。\n",
    "\n",
    "我们可以进行以下计算：\n",
    "\n",
    "- 第一个卷积层的参数量：(3×3×3)×64=1728(3×3×3)×64=1728\n",
    "- 后续卷积层的参数量：对于每个数量级的卷积层，参数量为 (3×3×𝐶in)×𝐶out，其中𝐶in是输入通道数，𝐶out是输出通道数。\n",
    "- 全连接层的参数量：第一层全连接层的参数量为 7×7×512×4096+4096（加上4096是因为偏置项），第二层为 4096×4096+4096，最后一层为 4096×1000+1000。\n",
    "\n",
    "<img src=\"./images/VGG16_params.png\" style=\"zoom:100%;\" />\n",
    "\n",
    "将所有卷积层和全连接层的参数量加起来，就可以得到VGG16模型的总参数量。VGG16的总参数量大约是138M（即138,357,544个参数）。这个数字包括了所有的权重和偏置参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.3 基于VGG16的土豆疾病识别\n",
    "\n",
    "<img src=\"./images/PotatoPlantDiseases.jpg\" style=\"zoom:100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.导入必需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.数据集的加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换操作\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "image_dataset = datasets.ImageFolder('./datasets/Potato Plant Diseases/PotatoPlants', train_transform)\n",
    "len(image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "split_size = int(0.8 * len(image_dataset))\n",
    "train_dataset, val_dataset = random_split(image_dataset, [split_size, len(image_dataset) - split_size])\n",
    "\n",
    "# 为测试集设置变换操作\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(64),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(64),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(128),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(128),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(256),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(256),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(256),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(512),  # BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            # nn.BatchNorm1d(4096),  # 1D BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(4096, 4096),\n",
    "            # nn.BatchNorm1d(4096),  # 1D BN\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(4096, 3)  # 输出层，3分类任务\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # 展开特征图\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VGG16().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型参数的初始化\n",
    "\n",
    "如果你没有手动初始化模型参数，PyTorch会使用其默认的参数初始化方法。对于不同的层和参数类型，PyTorch有不同的默认初始化策略：\n",
    "\n",
    "（1）权重（Weights）\n",
    "\n",
    "对于大多数线性层（nn.Linear）和卷积层（nn.Conv2d），如果没有指定初始化方法，PyTorch默认使用Kaiming He初始化（也称为He初始化），这是一种基于输入特征的初始化方法，适用于ReLU激活函数。\n",
    "\n",
    "（2）偏置（Bias）\n",
    "\n",
    "对于偏置参数，PyTorch默认将其初始化为0。\n",
    "\n",
    "（3）BatchNorm的参数\n",
    "\n",
    "对于批量归一化层（nn.BatchNorm），如果没有指定初始化方法，PyTorch默认将γ（缩放参数）初始化为1，将β（偏移参数）初始化为0。\n",
    "\n",
    "（4）其他层\n",
    "\n",
    "对于其他类型的层，如循环层（nn.LSTM、nn.GRU）等，PyTorch也有自己的默认初始化策略，通常是将权重初始化为较小的随机值，偏置初始化为0。\n",
    "\n",
    "默认初始化通常是一个好的起点，但根据你的具体任务和网络结构，可能需要进行自定义初始化以获得更好的性能。例如，如果你使用的是Sigmoid或Tanh激活函数，可能需要使用Xavier初始化（也称为Glorot初始化）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Loss: 1.0300155017111037\n",
      "Epoch [2/100] Loss: 0.9211143189006381\n",
      "Epoch [3/100] Loss: 0.9006726852169743\n",
      "Epoch [4/100] Loss: 0.8980600723513851\n",
      "Epoch [5/100] Loss: 0.8982197200810468\n",
      "Epoch [6/100] Loss: 0.8988692009890521\n",
      "Epoch [7/100] Loss: 0.8983504904641045\n",
      "Epoch [8/100] Loss: 0.8978953162829081\n",
      "Epoch [9/100] Loss: 0.8993810415267944\n",
      "Epoch [10/100] Loss: 0.8980289697647095\n",
      "Epoch [11/100] Loss: 0.8982789472297386\n",
      "Epoch [12/100] Loss: 0.8974555907426057\n",
      "Epoch [13/100] Loss: 0.8979859771551909\n",
      "Epoch [14/100] Loss: 0.898191511631012\n",
      "Epoch [15/100] Loss: 0.897787211117921\n",
      "Epoch [16/100] Loss: 0.8981168027277346\n",
      "Epoch [17/100] Loss: 0.8984847466150919\n",
      "Epoch [18/100] Loss: 0.8984142806794908\n",
      "Epoch [19/100] Loss: 0.8980646133422852\n",
      "Epoch [20/100] Loss: 0.8984923958778381\n",
      "Epoch [21/100] Loss: 0.8983448191925332\n",
      "Epoch [22/100] Loss: 0.8986719648043314\n",
      "Epoch [23/100] Loss: 0.8986277889322352\n",
      "Epoch [24/100] Loss: 0.8983955294997604\n",
      "Epoch [25/100] Loss: 0.8980541383778607\n",
      "Epoch [26/100] Loss: 0.8974933491812812\n",
      "Epoch [27/100] Loss: 0.8984868416079769\n",
      "Epoch [28/100] Loss: 0.8990599071538007\n",
      "Epoch [29/100] Loss: 0.8979825311236911\n",
      "Epoch [30/100] Loss: 0.898372232913971\n",
      "Epoch [31/100] Loss: 0.8981622280897917\n",
      "Epoch [32/100] Loss: 0.8983174584530018\n",
      "Epoch [33/100] Loss: 0.8985625041855706\n",
      "Epoch [34/100] Loss: 0.8990452444111859\n",
      "Epoch [35/100] Loss: 0.8987710807058547\n",
      "Epoch [36/100] Loss: 0.8983824849128723\n",
      "Epoch [37/100] Loss: 0.8978567587004768\n",
      "Epoch [38/100] Loss: 0.8998826852551213\n",
      "Epoch [39/100] Loss: 0.8984949103108159\n",
      "Epoch [40/100] Loss: 0.8980151723932337\n",
      "Epoch [41/100] Loss: 0.8983148402637906\n",
      "Epoch [42/100] Loss: 0.8985563384162055\n",
      "Epoch [43/100] Loss: 0.8981910568696482\n",
      "Epoch [44/100] Loss: 0.8983390816935787\n",
      "Epoch [45/100] Loss: 0.8980341001793191\n",
      "Epoch [46/100] Loss: 0.8981499296647532\n",
      "Epoch [47/100] Loss: 0.89845210313797\n",
      "Epoch [48/100] Loss: 0.8985734184583029\n",
      "Epoch [49/100] Loss: 0.8993135778992264\n",
      "Epoch [50/100] Loss: 0.8987804187668694\n",
      "Epoch [51/100] Loss: 0.8979803500352083\n",
      "Epoch [52/100] Loss: 0.8986301841559233\n",
      "Epoch [53/100] Loss: 0.8983728157149421\n",
      "Epoch [54/100] Loss: 0.8979196349779764\n",
      "Epoch [55/100] Loss: 0.8975406377403824\n",
      "Epoch [56/100] Loss: 0.8985862224190323\n",
      "Epoch [57/100] Loss: 0.8980537520514594\n",
      "Epoch [58/100] Loss: 0.8973947498533461\n",
      "Epoch [59/100] Loss: 0.8977905754689817\n",
      "Epoch [60/100] Loss: 0.8981588791917872\n",
      "Epoch [61/100] Loss: 0.8980416059494019\n",
      "Epoch [62/100] Loss: 0.8987873704345138\n",
      "Epoch [63/100] Loss: 0.8981997326568321\n",
      "Epoch [64/100] Loss: 0.8983589741918776\n",
      "Epoch [65/100] Loss: 0.8989939380575109\n",
      "Epoch [66/100] Loss: 0.8978953074525904\n",
      "Epoch [67/100] Loss: 0.8981443820176301\n",
      "Epoch [68/100] Loss: 0.8976503058716103\n",
      "Epoch [69/100] Loss: 0.8981941651414942\n",
      "Epoch [70/100] Loss: 0.8980821216547931\n",
      "Epoch [71/100] Loss: 0.8981411810274478\n",
      "Epoch [72/100] Loss: 0.898309208728649\n",
      "Epoch [73/100] Loss: 0.897499492874852\n",
      "Epoch [74/100] Loss: 0.8991911124300074\n",
      "Epoch [75/100] Loss: 0.8993388657216672\n",
      "Epoch [76/100] Loss: 0.8979333153477421\n",
      "Epoch [77/100] Loss: 0.8985783656438192\n",
      "Epoch [78/100] Loss: 0.8992226741932057\n",
      "Epoch [79/100] Loss: 0.8985594025364628\n",
      "Epoch [80/100] Loss: 0.8986645053934168\n",
      "Epoch [81/100] Loss: 0.8978661718191924\n",
      "Epoch [82/100] Loss: 0.8987012770440843\n",
      "Epoch [83/100] Loss: 0.8986935019493103\n",
      "Epoch [84/100] Loss: 0.8979262113571167\n",
      "Epoch [85/100] Loss: 0.8978655536969503\n",
      "Epoch [86/100] Loss: 0.8976257023987947\n",
      "Epoch [87/100] Loss: 0.8985206661400972\n",
      "Epoch [88/100] Loss: 0.8981106722796405\n",
      "Epoch [89/100] Loss: 0.8984934091567993\n",
      "Epoch [90/100] Loss: 0.8982029557228088\n",
      "Epoch [91/100] Loss: 0.8990269170867072\n",
      "Epoch [92/100] Loss: 0.8981032614354734\n",
      "Epoch [93/100] Loss: 0.8983511284545616\n",
      "Epoch [94/100] Loss: 0.8979726014313875\n",
      "Epoch [95/100] Loss: 0.8981519672605727\n",
      "Epoch [96/100] Loss: 0.8990151617262099\n",
      "Epoch [97/100] Loss: 0.8979051113128662\n",
      "Epoch [98/100] Loss: 0.8979513512717353\n",
      "Epoch [99/100] Loss: 0.898540011158696\n",
      "Epoch [100/100] Loss: 0.9003873003853692\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # # 打印每个批次的进度和损失\n",
    "        # if (i + 1) % 10 == 0:  # 每10个批次打印一次\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "    # 计算并打印平均损失\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss}\")\n",
    "\n",
    "torch.save(model,'weights/vgg16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.94%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print(f\"Accuracy: {(correct / total) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**批量归一化（Batch Normalization）的作用**\n",
    "\n",
    "训练深层神经⽹络是⼗分困难的，特别是在较短的时间内使他们收敛更加棘⼿。批量归一化（Batch Normalization, BN）最早由Sergey Ioffe和Christian Szegedy在2015年提出。这种技术被设计用来解决深度神经网络训练中的内部协变量偏移问题，并加速模型的收敛速度。BN的引入显著提高了深度神经网络的训练效率，并有助于提高模型的泛化能力，成为深度学习训练过程中的标准组件之一。\n",
    "\n",
    "⾸先，数据预处理的⽅式通常会对最终结果产⽣巨⼤影响。使⽤真实数据时，我们的第⼀步是标准化输⼊特征，使其平均值为0，⽅差为1。直观地说，这种标准化可以很好地与我们的优化器配合使⽤，因为它可以将参数的量级进⾏统⼀。\n",
    "\n",
    "第⼆，对于典型的多层感知机或卷积神经⽹络。当我们训练时，中间层中的变量可能具有更⼴的变化范围：不论是沿着从输⼊到输出的层，跨同⼀层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。批量标准化的发明者⾮正式地假设，这些变量分布中的这种偏移可能会阻碍⽹络的收敛。直观地说，我们可能会猜想，如果⼀个层的可变值是另⼀层的 100 倍，这可能需要对学习率进⾏补偿调整。\n",
    "\n",
    "第三，更深层的⽹络很复杂，容易过拟合。这意味着正则化变得更加重要。批量标准化应⽤于单个可选层（也可以应⽤到所有层），其原理如下：在每次训练迭代中，我们⾸先归⼀化输⼊，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应⽤⽐例系数和⽐例偏移。正是由于这个基于批量统计的标准化，才有了批量标准化的名称。\n",
    "\n",
    "总结起来批量归一化的作用有以下三点：\n",
    "\n",
    "（1）加快网络训练的收敛速度；\n",
    "\n",
    "（2）控制梯度爆炸/防止梯度消失；\n",
    "\n",
    "（3）防止过拟合。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
