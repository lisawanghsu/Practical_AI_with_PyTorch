{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT（2018年）\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer架构的预训练语言模型，由Google在2018年提出。它在自然语言处理（NLP）领域取得了革命性的进展，广泛应用于各种任务，如文本分类、命名实体识别、问答系统等。BERT 的核心创新是其训练方法，它允许模型学习到双向的上下文信息，这与传统的单向语言模型（如 Word2Vec 或 GloVe）相比是一个显著的进步。BERT 的提出标志着 NLP 领域的一个重要里程碑，它推动了预训练语言模型的发展，并为各种 NLP 任务提供了强大的基础。\n",
    "\n",
    "### **1. 核心特点**\n",
    "\n",
    "- **双向编码器**：BERT通过Transformer的编码器结构，能够同时考虑上下文信息，而不仅仅是单向的（如传统的RNN或LSTM）。这使得BERT能够更好地理解语言的语义。\n",
    "- **预训练+微调**：BERT采用两阶段训练方式：\n",
    "  1. **预训练阶段**：在大规模无标签文本数据上进行训练，学习通用的语言表示。\n",
    "  2. **微调阶段**：在特定任务上使用少量标注数据进行微调，以适应具体任务。\n",
    "- **多任务学习**：BERT在预训练阶段使用了两个任务：\n",
    "  1. **Masked Language Model (MLM)**：随机遮蔽输入文本中的某些词，模型需要预测被遮蔽的词。\n",
    "  2. **Next Sentence Prediction (NSP)**：判断两个句子是否是连续的上下文关系。\n",
    "\n",
    "### **2. 模型架构**\n",
    "\n",
    "- **Transformer编码器**：BERT基于Transformer的编码器部分，具有多头自注意力机制和前馈神经网络。\n",
    "- **多层结构**：BERT有多个编码器层（如BERT-base有12层，BERT-large有24层），每层都包含自注意力机制和前馈网络。\n",
    "- **输入表示**：\n",
    "  - 输入是token序列，通常使用WordPiece分词。\n",
    "  - 输入包括三个部分：\n",
    "    1. **Token Embeddings**：词嵌入。\n",
    "    2. **Segment Embeddings**：用于区分句子对中的两个句子。\n",
    "    3. **Position Embeddings**：用于表示词的位置信息。\n",
    "\n",
    "### **3. 预训练任务**\n",
    "\n",
    "- **Masked Language Model (MLM)**：\n",
    "  - 随机遮蔽输入序列中的15%的词。\n",
    "  - 模型需要预测被遮蔽的词。\n",
    "  - 这种任务使得模型能够学习到上下文信息。\n",
    "- **Next Sentence Prediction (NSP)**：\n",
    "  - 输入是两个句子，模型需要判断第二个句子是否是第一个句子的下一句。\n",
    "  - 这种任务帮助模型理解句子之间的关系。\n",
    "\n",
    "### **4. 微调阶段**\n",
    "\n",
    "- 在特定任务上，BERT的输出层会被替换为任务相关的输出层（如分类层、序列标注层等）。\n",
    "- 使用少量标注数据对整个模型进行微调，以适应具体任务。\n",
    "\n",
    "### **5. 优势**\n",
    "\n",
    "- **强大的语言理解能力**：BERT能够捕捉复杂的语言模式，因为它同时考虑了上下文信息。\n",
    "- **广泛适用性**：BERT可以应用于多种NLP任务，如文本分类、情感分析、问答系统等。\n",
    "- **高性能**：在多个NLP基准测试中，BERT表现优异，显著超越了之前的模型。\n",
    "\n",
    "### **6. 变体**\n",
    "\n",
    "- **BERT-base**：12层Transformer编码器，768维隐藏层，12个注意力头。\n",
    "- **BERT-large**：24层Transformer编码器，1024维隐藏层，16个注意力头。\n",
    "- **其他变体**：\n",
    "  - **RoBERTa**：改进了BERT的预训练策略，去掉了NSP任务，增加了训练数据和训练步数。\n",
    "  - **ALBERT**：通过参数共享和更小的嵌入层，减少了模型的参数量。\n",
    "  - **DistilBERT**：通过知识蒸馏，构建了一个更小的BERT模型，但保留了大部分性能。\n",
    "\n",
    "### **7. 应用场景**\n",
    "\n",
    "- **文本分类**：如情感分析、垃圾邮件检测。\n",
    "- **命名实体识别**：如提取文本中的地名、人名等。\n",
    "- **问答系统**：如SQuAD数据集上的问答任务。\n",
    "- **机器翻译**：虽然BERT本身不是为翻译设计，但可以作为特征提取器。\n",
    "- **文本生成**：结合生成模型（如GPT）进行文本生成任务。\n",
    "\n",
    "BERT通过双向Transformer编码器和创新的预训练任务，显著提升了自然语言处理任务的性能。它的成功推动了后续一系列基于Transformer的模型（如GPT、T5、BART等）的发展，成为现代NLP领域的基石。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目：商品评论情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# 检查是否有可用的GPU，如果有则使用GPU，否则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x239aa63c730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 123\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件内容的函数\n",
    "def data_read(filepath):\n",
    "    # 打开指定路径的文件，使用gb18030编码，读取每一行并去除行尾的空白字符\n",
    "    data_raw = [line.strip() for line in open(filepath, encoding='gb18030')]\n",
    "    return data_raw  # 返回读取到的数据列表\n",
    "\n",
    "# 合并数据的函数，save_flag参数用于控制是否保存合并后的数据\n",
    "def merge_data(save_flag=None):\n",
    "    # 定义需要处理的文件名列表\n",
    "    files_name = ['hotel']\n",
    "    # 初始化正向文本和负向文本的列表\n",
    "    pos_text, neg_text = [], []\n",
    "    # 遍历文件名列表\n",
    "    for filename in files_name:\n",
    "        # 读取正向文本文件\n",
    "        pos_temp = data_read('./datasets/txt_cls/' + filename + '/pos.txt')\n",
    "        # 读取负向文本文件\n",
    "        neg_temp = data_read('./datasets/txt_cls/' + filename + '/neg.txt')\n",
    "        # 将读取到的正向文本添加到pos_text列表中\n",
    "        pos_text.extend(pos_temp)\n",
    "        # 将读取到的负向文本添加到neg_text列表中\n",
    "        neg_text.extend(neg_temp)\n",
    "    # 如果save_flag为True，则将合并后的正向和负向文本保存到指定文件中\n",
    "    if save_flag:\n",
    "        write_txt('./datasets/txt_cls/pos_all.txt', pos_text)\n",
    "        write_txt('./datasets/txt_cls/neg_all.txt', neg_text)\n",
    "    # 返回合并后的正向和负向文本列表\n",
    "    return pos_text, neg_text\n",
    "\n",
    "# 调用merge_data函数，获取正向和负向文本数据\n",
    "pos_text, neg_text = merge_data()\n",
    "# 将正向和负向文本数据合并为一个总的句子列表\n",
    "sentences = pos_text + neg_text\n",
    "\n",
    "# 设定标签，正向文本标签为0，负向文本标签为1\n",
    "pos_targets = np.zeros((len(pos_text)))  # 正向文本标签数组，长度与pos_text相同，值全为0\n",
    "neg_targets = np.ones((len(neg_text)))   # 负向文本标签数组，长度与neg_text相同，值全为1\n",
    "# 将正向和负向标签数组拼接在一起，并调整形状为(n, 1)\n",
    "targets = np.concatenate((pos_targets, neg_targets), axis=0).reshape(-1, 1)  # (10000, 1)\n",
    "# 将标签数组转换为PyTorch张量\n",
    "total_targets = torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不错，下次还考虑入住。交通也方便，在餐厅吃的也不错。\n",
      "['不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。']\n",
      "[101, 679, 7231, 8024, 678, 3613, 6820, 5440, 5991, 1057, 857, 511, 769, 6858, 738, 3175, 912, 8024, 1762, 7623, 1324, 1391, 4638, 738, 679, 7231, 511, 102]\n",
      "['[CLS]', '不', '错', '，', '下', '次', '还', '考', '虑', '入', '住', '。', '交', '通', '也', '方', '便', '，', '在', '餐', '厅', '吃', '的', '也', '不', '错', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 从预训练的BERT模型权重中加载中文BERT的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('./weights/bert-base-chinese')\n",
    "\n",
    "# 打印正向文本数据中的第3条文本（索引为2）\n",
    "print(pos_text[2])\n",
    "\n",
    "# 使用分词器对第3条文本进行分词，并打印分词结果\n",
    "print(tokenizer.tokenize(pos_text[2]))\n",
    "\n",
    "# 使用分词器对第3条文本进行编码（转换为BERT模型的输入ID），并打印编码结果\n",
    "print(tokenizer.encode(pos_text[2]))\n",
    "\n",
    "# 使用分词器将编码后的ID转换回分词形式，并打印结果\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(pos_text[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 52])\n",
      "tensor([[ 101, 6983, 2421,  ..., 2791, 3198,  102],\n",
      "        [ 101, 1765, 4415,  ..., 7313, 7599,  102],\n",
      "        [ 101,  679, 7231,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2769, 2697,  ...,    0,    0,    0],\n",
      "        [ 101, 2791, 7313,  ..., 1218, 1447,  102],\n",
      "        [ 101, 5439,  782,  ...,  102,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# 将每一句文本转换为数字编码的函数\n",
    "def convert_text_to_token(tokenizer, sentence, limit_size=50):\n",
    "    # 使用分词器对句子进行编码，并截断到limit_size长度\n",
    "    tokens = tokenizer.encode(sentence[:limit_size])\n",
    "    # 如果编码后的长度小于limit_size + 2（加上首尾标识符），则用0补齐\n",
    "    if len(tokens) < limit_size + 2:\n",
    "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
    "    return tokens  # 返回处理后的编码列表\n",
    "\n",
    "# 对所有句子进行编码转换，生成输入ID列表\n",
    "input_ids = [convert_text_to_token(tokenizer, sen) for sen in sentences]\n",
    "# 将输入ID列表转换为PyTorch张量\n",
    "input_tokens = torch.tensor(input_ids)\n",
    "\n",
    "# 打印输入张量的形状\n",
    "print(input_tokens.shape)  # 输出形状为 [10000, 52]，表示10000个句子，每个句子长度为52\n",
    "# 打印输入张量的内容\n",
    "print(input_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建注意力掩码的函数\n",
    "def attention_masks(input_ids):\n",
    "    atten_masks = []  # 初始化注意力掩码列表\n",
    "    # 遍历输入ID列表\n",
    "    for seq in input_ids:\n",
    "        # 生成当前句子的注意力掩码\n",
    "        # 如果当前位置的ID大于0（表示不是PAD符号），则为1，否则为0\n",
    "        seq_mask = [float(i > 0) for i in seq]\n",
    "        atten_masks.append(seq_mask)  # 将当前句子的掩码添加到列表中\n",
    "    return atten_masks  # 返回注意力掩码列表\n",
    "\n",
    "# 调用函数生成注意力掩码\n",
    "atten_masks = attention_masks(input_ids)\n",
    "# 将注意力掩码列表转换为PyTorch张量\n",
    "attention_tokens = torch.tensor(atten_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 52]) torch.Size([2000, 52])\n",
      "torch.Size([8000, 52])\n",
      "tensor([  101,  2769,  6370,  4638,  3221, 10189,  1039,  4638,   117,   852,\n",
      "         2769,  6230,  2533,  8821,  1039,  4638,  7599,  3419,  3291,  1962,\n",
      "          671,   763,   117,  3300,   671,  2476,  1377,   809,  1288,  1309,\n",
      "         4638,  3763,  1355,   119,  2456,  6379,  1920,  2157,  6370,  3249,\n",
      "         6858,  7313,   106,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和测试集\n",
    "# 将输入数据和标签数据按照8:2的比例划分为训练集和测试集\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    input_tokens, total_targets, random_state=666, test_size=0.2)\n",
    "\n",
    "# 同时划分注意力掩码数据，保持与输入数据的一致性\n",
    "train_masks, test_masks, _, _ = train_test_split(\n",
    "    attention_tokens, input_tokens, random_state=666, test_size=0.2)\n",
    "\n",
    "# 打印训练集和测试集输入数据的形状\n",
    "print(train_inputs.shape, test_inputs.shape)  # 输出训练集和测试集输入数据的形状\n",
    "\n",
    "# 打印训练集注意力掩码的形状\n",
    "print(train_masks.shape)  # 输出训练集注意力掩码的形状\n",
    "\n",
    "# 打印训练集第一个样本的输入数据\n",
    "print(train_inputs[0])\n",
    "\n",
    "# 打印训练集第一个样本的注意力掩码\n",
    "print(train_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "BATCH_SIZE = 4  # 批处理大小\n",
    "LEARNING_RATE = 2e-5  # 学习率\n",
    "WEIGHT_DECAY = 1e-2  # 权重衰减（L2正则化）\n",
    "EPSILON = 1e-8  # 优化器中的epsilon参数，用于数值稳定性\n",
    "\n",
    "# 创建训练集的TensorDataset\n",
    "# 将输入数据、注意力掩码和标签打包成一个数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "\n",
    "# 创建训练集的随机采样器\n",
    "# 随机打乱训练数据，以提高训练效果\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# 创建训练集的DataLoader\n",
    "# 使用随机采样器和批处理大小，将数据分批加载\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 创建测试集的TensorDataset\n",
    "# 将输入数据、注意力掩码和标签打包成一个数据集\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# 创建测试集的顺序采样器\n",
    "# 按顺序加载测试数据，保持数据顺序不变\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "# 创建测试集的DataLoader\n",
    "# 使用顺序采样器和批处理大小，将数据分批加载\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 52]) torch.Size([4, 52]) torch.Size([4, 1])\n",
      "len(train_dataloader)= 2000\n"
     ]
    }
   ],
   "source": [
    "# 遍历训练集的DataLoader，获取一个批次的数据\n",
    "for i, (train, mask, label) in enumerate(train_dataloader):\n",
    "    # 打印当前批次中输入数据、注意力掩码和标签的形状\n",
    "    print(train.shape, mask.shape, label.shape)  # 输出形状为 [BATCH_SIZE, 52], [BATCH_SIZE, 52], [BATCH_SIZE, 1]\n",
    "    break  # 只取第一个批次的数据，然后退出循环\n",
    "\n",
    "# 打印训练集DataLoader的长度，即批次数\n",
    "print('len(train_dataloader)=', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./weights/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从预训练的BERT模型权重中加载用于序列分类的BERT模型\n",
    "# 设置num_labels为2，表示这是一个二分类任务（好评和差评）\n",
    "model = BertForSequenceClassification.from_pretrained(\"./weights/bert-base-chinese\", num_labels=2)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "# 使用AdamW优化器，适用于BERT模型的优化\n",
    "# 参数lr为学习率，eps为数值稳定性参数（默认值为1e-8）\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)\n",
    "\n",
    "# 定义训练的轮数\n",
    "epochs = 2\n",
    "\n",
    "# 计算总的训练步数\n",
    "# 总步数 = 批次数 * 训练轮数\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 设计学习率调度器\n",
    "# 使用线性学习率调度器，包含预热阶段\n",
    "# num_warmup_steps为预热步数，num_training_steps为总训练步数\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义二分类准确率的计算函数\n",
    "def binary_acc(preds, labels):  # preds.shape=(4, 2) labels.shape=torch.Size([4, 1])\n",
    "    # 计算预测结果的最大值索引（即预测的类别）\n",
    "    # torch.max(preds, dim=1)[1] 返回每行最大值的索引，shape为 [4]\n",
    "    # labels.flatten() 将标签展平为一维张量，shape为 [4]\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()\n",
    "    # 计算准确率：正确预测的数量 / 总样本数\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc  # 返回准确率\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # 将时间四舍五入为整数\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # 将时间格式化为 hh:mm:ss 形式\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, optimizer):\n",
    "    t0 = time.time()  # 记录训练开始时间\n",
    "    avg_loss, avg_acc = [], []  # 初始化平均损失和平均准确率列表\n",
    "\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "\n",
    "    # 遍历训练数据加载器中的每个批次\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 每隔100个批次，输出当前的训练进度和已用时间\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    time: {elapsed:}.')\n",
    "\n",
    "        # 将批次数据移动到指定设备（GPU或CPU）\n",
    "        b_input_ids, b_input_mask, b_labels = (\n",
    "            batch[0].long().to(device),  # 输入ID\n",
    "            batch[1].long().to(device),  # 注意力掩码\n",
    "            batch[2].long().to(device)   # 标签\n",
    "        )\n",
    "\n",
    "        # 前向传播：获取模型的输出\n",
    "        output = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,  # 对于BERT模型，token_type_ids通常为None\n",
    "            attention_mask=b_input_mask,  # 注意力掩码\n",
    "            labels=b_labels  # 真实标签\n",
    "        )\n",
    "        loss, logits = output[0], output[1]  # 获取损失和预测结果\n",
    "\n",
    "        avg_loss.append(loss.item())  # 记录当前批次的损失\n",
    "\n",
    "        # 计算当前批次的准确率\n",
    "        acc = binary_acc(logits, b_labels)\n",
    "        avg_acc.append(acc)  # 记录当前批次的准确率\n",
    "\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        loss.backward()  # 反向传播，计算梯度\n",
    "        clip_grad_norm_(model.parameters(), 1.0)  # 梯度裁剪，防止梯度爆炸\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        scheduler.step()  # 更新学习率\n",
    "\n",
    "    # 计算平均准确率和平均损失\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "\n",
    "    return avg_loss, avg_acc  # 返回平均损失和平均准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def evaluate(model):\n",
    "    avg_acc = []  # 初始化平均准确率列表\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "    # 禁用梯度计算，以节省内存和提高速度\n",
    "    with torch.no_grad():\n",
    "        # 遍历测试数据加载器中的每个批次\n",
    "        for batch in test_dataloader:\n",
    "            # 将批次数据移动到指定设备（GPU或CPU）\n",
    "            b_input_ids, b_input_mask, b_labels = (\n",
    "                batch[0].long().to(device),  # 输入ID\n",
    "                batch[1].long().to(device),  # 注意力掩码\n",
    "                batch[2].long().to(device)   # 标签\n",
    "            )\n",
    "\n",
    "            # 前向传播：获取模型的输出\n",
    "            output = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,  # 对于BERT模型，token_type_ids通常为None\n",
    "                attention_mask=b_input_mask  # 注意力掩码\n",
    "            )\n",
    "\n",
    "            # 计算当前批次的准确率\n",
    "            acc = binary_acc(output[0], b_labels)\n",
    "            avg_acc.append(acc)  # 记录当前批次的准确率\n",
    "\n",
    "    # 计算平均准确率\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc  # 返回平均准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  2,000.    time: 0:00:05.\n",
      "  Batch   200  of  2,000.    time: 0:00:09.\n",
      "  Batch   300  of  2,000.    time: 0:00:14.\n",
      "  Batch   400  of  2,000.    time: 0:00:18.\n",
      "  Batch   500  of  2,000.    time: 0:00:22.\n",
      "  Batch   600  of  2,000.    time: 0:00:27.\n",
      "  Batch   700  of  2,000.    time: 0:00:31.\n",
      "  Batch   800  of  2,000.    time: 0:00:35.\n",
      "  Batch   900  of  2,000.    time: 0:00:40.\n",
      "  Batch 1,000  of  2,000.    time: 0:00:44.\n",
      "  Batch 1,100  of  2,000.    time: 0:00:48.\n",
      "  Batch 1,200  of  2,000.    time: 0:00:53.\n",
      "  Batch 1,300  of  2,000.    time: 0:00:57.\n",
      "  Batch 1,400  of  2,000.    time: 0:01:01.\n",
      "  Batch 1,500  of  2,000.    time: 0:01:06.\n",
      "  Batch 1,600  of  2,000.    time: 0:01:10.\n",
      "  Batch 1,700  of  2,000.    time: 0:01:15.\n",
      "  Batch 1,800  of  2,000.    time: 0:01:19.\n",
      "  Batch 1,900  of  2,000.    time: 0:01:24.\n",
      "epoch=0, train_acc=0.87825, loss=0.4390542203724908\n",
      "epoch=0, test_acc=0.9135\n",
      "  Batch   100  of  2,000.    time: 0:00:04.\n",
      "  Batch   200  of  2,000.    time: 0:00:09.\n",
      "  Batch   300  of  2,000.    time: 0:00:13.\n",
      "  Batch   400  of  2,000.    time: 0:00:18.\n",
      "  Batch   500  of  2,000.    time: 0:00:22.\n",
      "  Batch   600  of  2,000.    time: 0:00:26.\n",
      "  Batch   700  of  2,000.    time: 0:00:31.\n",
      "  Batch   800  of  2,000.    time: 0:00:35.\n",
      "  Batch   900  of  2,000.    time: 0:00:39.\n",
      "  Batch 1,000  of  2,000.    time: 0:00:43.\n",
      "  Batch 1,100  of  2,000.    time: 0:00:48.\n",
      "  Batch 1,200  of  2,000.    time: 0:00:52.\n",
      "  Batch 1,300  of  2,000.    time: 0:00:57.\n",
      "  Batch 1,400  of  2,000.    time: 0:01:01.\n",
      "  Batch 1,500  of  2,000.    time: 0:01:05.\n",
      "  Batch 1,600  of  2,000.    time: 0:01:10.\n",
      "  Batch 1,700  of  2,000.    time: 0:01:14.\n",
      "  Batch 1,800  of  2,000.    time: 0:01:19.\n",
      "  Batch 1,900  of  2,000.    time: 0:01:23.\n",
      "epoch=1, train_acc=0.944, loss=0.24964090898186986\n",
      "epoch=1, test_acc=0.9195\n"
     ]
    }
   ],
   "source": [
    "# 训练和评估的主循环\n",
    "for epoch in range(epochs):\n",
    "    # 训练模型\n",
    "    train_loss, train_acc = train(model, optimizer)\n",
    "    # 打印当前训练轮次的训练准确率和损失\n",
    "    print('epoch={}, train_acc={}, loss={}'.format(epoch, train_acc, train_loss))\n",
    "    \n",
    "    # 评估模型在测试集上的表现\n",
    "    test_acc = evaluate(model)\n",
    "    # 打印当前训练轮次的测试准确率\n",
    "    print(\"epoch={}, test_acc={}\".format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "def predict(sen):\n",
    "    # 将输入文本转换为BERT模型的输入ID\n",
    "    input_id = convert_text_to_token(tokenizer, sen)\n",
    "    # 将输入ID转换为PyTorch张量，并移动到指定设备（GPU或CPU）\n",
    "    input_token = torch.tensor(input_id).long().to(device)  # torch.Size([128])\n",
    "    \n",
    "    # 创建注意力掩码\n",
    "    atten_mask = [float(i > 0) for i in input_id]\n",
    "    # 将注意力掩码转换为PyTorch张量，并移动到指定设备（GPU或CPU）\n",
    "    attention_token = torch.tensor(atten_mask).long().to(device)  # torch.Size([128])\n",
    "    \n",
    "    # 前向传播：获取模型的输出\n",
    "    # 将输入张量和注意力掩码调整为形状 [1, 128]，以匹配模型的输入要求\n",
    "    output = model(\n",
    "        input_token.view(1, -1),  # 输入ID\n",
    "        token_type_ids=None,      # 对于BERT模型，token_type_ids通常为None\n",
    "        attention_mask=attention_token.view(1, -1)  # 注意力掩码\n",
    "    )\n",
    "    \n",
    "    # 打印模型的输出（预测结果）\n",
    "    print(output[0])\n",
    "    \n",
    "    # 返回预测结果的最大值索引（即预测的类别）\n",
    "    return torch.max(output[0], dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.4393,  4.0700]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "差评\n"
     ]
    }
   ],
   "source": [
    "label = predict('你家的外卖狗都不吃！')\n",
    "print('好评' if label==0 else '差评')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "runLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
