{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.Transformer组件\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 能用代码实现编码器、解码器、编码器-解码器结构\n",
    "\n",
    "2. 能用代码实现基于正弦余弦函数的位置编码\n",
    "\n",
    "3. 能用代码实现基于位置的前馈网络（FFN）\n",
    "\n",
    "4. 能用代码实现残差连接和层规范化(Add&Norm)结构\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种深度学习模型，由 Vaswani 等人在 2017 年的论文《Attention Is All You Need》中首次提出。它在自然语言处理（NLP）领域取得了革命性的进展。以下是 Transformer 模型的几个关键特点：\n",
    "\n",
    "自注意力机制（Self-Attention）：Transformer 摒弃了传统的循环神经网络（RNN）结构，使用自注意力机制来捕捉序列内不同位置之间的依赖关系。这种机制允许模型同时处理序列中的所有元素，从而提高了计算效率。\n",
    "\n",
    "并行处理能力：由于自注意力机制的特性，Transformer 可以并行处理序列中的所有元素，这与传统的序列模型（如 LSTM 或 GRU）相比，大大提高了训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer的编码器-解码器架构\n",
    "\n",
    "Transformer 模型通常由编码器（Encoder）和解码器（Decoder）组成。编码器将输入序列转换为连续的表示，而解码器则使用这些表示来生成输出序列。\n",
    "\n",
    "多头注意力（Multi-Head Attention）：Transformer 通过多头注意力机制，允许模型同时从不同的表示子空间捕捉信息，增强了模型的表达能力。\n",
    "\n",
    "位置编码（Positional Encoding）：由于 Transformer 模型本身不具备捕捉序列顺序的能力，因此引入了位置编码来提供序列中每个元素的位置信息。\n",
    "\n",
    "层归一化（Layer Normalization）和残差连接（Residual Connections）：Transformer 使用层归一化和残差连接来促进深层网络的训练，防止梯度消失或爆炸问题。\n",
    "\n",
    "<img src=\"./images/transformer.jpg\" style=\"zoom:60%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练和微调：Transformer 模型通常在大量数据上进行预训练，学习通用的语言表示，然后可以在特定任务上进行微调。\n",
    "\n",
    "广泛的应用：Transformer 不仅在机器翻译上表现出色，还被广泛应用于其他 NLP 任务，如文本摘要、问答系统、文本分类等。\n",
    "\n",
    "变体和发展：自原始 Transformer 提出以来，出现了许多改进和变体，如 BERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-trained Transformer）等，这些模型在不同的任务上都取得了显著的成果。\n",
    "\n",
    "Transformer 模型的提出，标志着 NLP 领域的一个重要转折点，它为处理序列数据提供了一种全新的视角和强大的工具。\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.位置编码\n",
    "\n",
    "由于Transformer模型本身不具有处理序列顺序的能力，位置编码使得模型能够理解单词在句子中的相对位置。\n",
    "\n",
    "我们可以使用不同频率的正弦和余弦函数为序列中的每个位置生成唯一的编码。位置编码的公式如下：\n",
    "\n",
    "<img src=\"./images/PE.jpg\" style=\"zoom:100%;\" />\n",
    "\n",
    "其中：\n",
    "\n",
    "𝑃𝐸是位置编码矩阵。\n",
    "\n",
    "pos是词在序列中的绝对位置（从0开始）。\n",
    "\n",
    "𝑖是维度索引（从0开始）。\n",
    "\n",
    "𝑑model是模型的维度大小。\n",
    "\n",
    "对于每个维度𝑖，位置编码包含两个值：一个正弦值和一个余弦值，分别对应偶数索引和奇数索引。\n",
    "\n",
    "位置编码的目的是给模型提供每个词在序列中的相对位置信息，这样模型就可以利用这些信息来理解词与词之间的关系。位置编码通常被添加到词嵌入（Word Embeddings）中，然后一起输入到Transformer模型中。\n",
    "\n",
    "Transformer的位置编码选择三角函数的官方解释是：\n",
    "\n",
    "位置编码的每个维度都对应于一个正弦曲线。波长形成一个从2π到10000·2π的几何轨迹。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置进行学习，因为对于任何固定的偏移量k,PEpos+k都可以表示为PEpos的线性函数。\n",
    "\n",
    "也就是说，每个维度都是波长不同的正弦波，波长范围是2π到10000·2π，选用10000这个比较大的数是因为三角函数式有周期的，在这个范围基本上，就不会出现波长一样的情况了。然后谷歌的科学家们为了让PE值的周期更长，还交替使用sin/cos来计算PE的值，就得到了最终的公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 初始化一个全0的位置编码矩阵，大小为(1, max_len, num_hiddens)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        # 计算位置编码\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        # 使用正弦函数为0,2,4,...维度的位置上填充位置编码\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        # 使用余弦函数为1,3,5,...维度的位置上填充位置编码\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 将位置编码加到输入X上\n",
    "        X += self.P[:, :X.shape[1], :].to(X.device)\n",
    "        \n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.基于位置的前馈网络（FFN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于位置的前馈网络\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.残差连接和层规范化(Add&Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差连接和层规范化(Add&Norm)\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.编码器-解码器结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 编码器\n",
    "\n",
    "在编码器接口中，我们只指定长度可变的序列作为编码器的输入X。任何继承这个Encoder基类的模型将完成代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 解码器\n",
    "\n",
    "在下面的解码器接口中，我们新增一个init_state函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入和编码后的状态映射成当前时间步的输出词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 初始化方法，在这里可以初始化解码器的参数和层\n",
    "        super().__init__()\n",
    "\n",
    "    # init_state方法用于初始化解码器的状态。\n",
    "    # 这个方法应该在每个具体的解码器类中被实现（override）。\n",
    "    # enc_all_outputs参数代表编码器的所有输出，可以用于初始化解码器的状态。\n",
    "    # *args允许传入额外的参数，这提供了灵活性，以适应不同的解码器实现。\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # X代表输入到解码器的序列数据。\n",
    "    # state代表解码器的状态，它可能包含编码器的输出、隐藏状态等信息。\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 编码器-解码器结构\n",
    "\n",
    "编码器‐解码器结构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
