{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.Transformer组件\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 能用代码实现编码器、解码器、编码器-解码器结构\n",
    "\n",
    "2. 能用代码实现基于正弦余弦函数的位置编码\n",
    "\n",
    "3. 能用代码实现基于位置的前馈网络（FFN）\n",
    "\n",
    "4. 能用代码实现残差连接和层规范化(Add&Norm)结构\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 是一种深度学习模型，由 Vaswani 等人在 2017 年的论文《Attention Is All You Need》中首次提出。它在自然语言处理（NLP）领域取得了革命性的进展。以下是 Transformer 模型的几个关键特点：\n",
    "\n",
    "自注意力机制（Self-Attention）：Transformer 摒弃了传统的循环神经网络（RNN）结构，使用自注意力机制来捕捉序列内不同位置之间的依赖关系。这种机制允许模型同时处理序列中的所有元素，从而提高了计算效率。\n",
    "\n",
    "并行处理能力：由于自注意力机制的特性，Transformer 可以并行处理序列中的所有元素，这与传统的序列模型（如 LSTM 或 GRU）相比，大大提高了训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer的编码器-解码器架构\n",
    "\n",
    "Transformer 模型通常由编码器（Encoder）和解码器（Decoder）组成。编码器将输入序列转换为连续的表示，而解码器则使用这些表示来生成输出序列。\n",
    "\n",
    "多头注意力（Multi-Head Attention）：Transformer 通过多头注意力机制，允许模型同时从不同的表示子空间捕捉信息，增强了模型的表达能力。\n",
    "\n",
    "位置编码（Positional Encoding）：由于 Transformer 模型本身不具备捕捉序列顺序的能力，因此引入了位置编码来提供序列中每个元素的位置信息。\n",
    "\n",
    "层归一化（Layer Normalization）和残差连接（Residual Connections）：Transformer 使用层归一化和残差连接来促进深层网络的训练，防止梯度消失或爆炸问题。\n",
    "\n",
    "<img src=\"./images/transformer.jpg\" style=\"zoom:60%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了能够顺利构建出Transformer模型，我们先来构建Transformer的几个核心组件：\n",
    "\n",
    "- 位置编码（Positional Encoding）\n",
    "- 基于位置的前馈网络（Feed Forward Network）\n",
    "- 残差连接和层规范化(Add&Norm)\n",
    "- 编码器（Encoder）\n",
    "- 解码器（Decoder）\n",
    "- 编码器-解码器（Encoder-Decoder）\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.位置编码\n",
    "\n",
    "由于Transformer模型本身不具有处理序列顺序的能力，位置编码使得模型能够理解单词在句子中的相对位置。\n",
    "\n",
    "我们可以使用不同频率的正弦和余弦函数为序列中的每个位置生成唯一的编码。位置编码的公式如下：\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "𝑃𝐸是位置编码矩阵。\n",
    "\n",
    "pos是词在序列中的绝对位置（从0开始）。\n",
    "\n",
    "𝑖是维度索引（从0开始）。\n",
    "\n",
    "𝑑model是模型的维度大小。\n",
    "\n",
    "对于每个维度𝑖，位置编码包含两个值：一个正弦值和一个余弦值，分别对应偶数索引和奇数索引。\n",
    "\n",
    "位置编码的目的是给模型提供每个词在序列中的相对位置信息，这样模型就可以利用这些信息来理解词与词之间的关系。位置编码通常被添加到词嵌入（Word Embeddings）中，然后一起输入到Transformer模型中。\n",
    "\n",
    "Transformer的位置编码选择三角函数的官方解释是：\n",
    "\n",
    "位置编码的每个维度都对应于一个正弦曲线。波长形成一个从2π到10000·2π的几何轨迹。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置进行学习，因为对于任何固定的偏移量k,PEpos+k都可以表示为PEpos的线性函数。\n",
    "\n",
    "也就是说，每个维度都是波长不同的正弦波，波长范围是2π到10000·2π，选用10000这个比较大的数是因为三角函数式有周期的，在这个范围基本上，就不会出现波长一样的情况了。然后谷歌的科学家们为了让PE值的周期更长，还交替使用sin/cos来计算PE的值，就得到了最终的公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 初始化一个全0的位置编码矩阵，大小为(1, max_len, num_hiddens)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        # 计算位置编码\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        # 使用正弦函数为0,2,4,...维度的位置上填充位置编码\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        # 使用余弦函数为1,3,5,...维度的位置上填充位置编码\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 将位置编码加到输入X上\n",
    "        X += self.P[:, :X.shape[1], :].to(X.device)\n",
    "        \n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.基于位置的前馈网络（FFN）\n",
    "\n",
    "这就是一个简单的MLP，它由两个全连接层组成。这两个全连接层的输入是来自前一层的输出和来自位置编码的向量。位置编码向量是一种映射，它将位置信息编码到输入向量中，使得模型能够捕捉到输入序列中各个位置之间的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于位置的前馈网络\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.残差连接和层规范化(Add&Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）什么是层规范化？\n",
    "\n",
    "层规范化（Layer Normalization, LayerNorm）是一种用于神经网络的归一化技术，主要用于稳定神经网络的训练过程。它与批量规范化（Batch Normalization, BatchNorm）类似，但适用的场景和实现方式有所不同。层规范化是对输入张量的**每个样本**进行归一化，而不是对整个批次进行归一化。具体来说，它对输入张量的**最后一个维度**（通常是特征维度）进行归一化，使得每个样本的特征分布更加稳定。\n",
    "\n",
    "（2）层规范化的计算公式\n",
    "\n",
    "对于输入张量x，层规范化的计算公式如下：\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\left( \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\right) + \\beta\n",
    "$$\n",
    "其中：\n",
    "\n",
    "-  x ：输入张量，形状为  (N, *) ，其中  N  是样本数，*表示任意其他维度。\n",
    "\n",
    "-  mu：输入张量在最后一个维度上的均值，计算公式为：\n",
    "  $$\n",
    "  \\mu = \\frac{1}{H} \\sum_{i=1}^H x_i\n",
    "  $$\n",
    "  其中  H  是最后一个维度的大小。\n",
    "\n",
    "- sigma^2 ：输入张量在最后一个维度上的方差，计算公式为：\n",
    "  $$\n",
    "  \\sigma^2 = \\frac{1}{H} \\sum_{i=1}^H (x_i - \\mu)^2\n",
    "  $$\n",
    "  \n",
    "\n",
    "- epsilon：一个很小的常数，用于防止分母为零。\n",
    "\n",
    "- gamma  和 beta ：可学习的缩放因子和偏移因子，形状与输入张量的最后一个维度相同。\n",
    "\n",
    "（3）层规范化与批量规范化的对比\n",
    "\n",
    "| 特性           | 批量规范化（BatchNorm）          | 层规范化（LayerNorm）                |\n",
    "| -------------- | -------------------------------- | ------------------------------------ |\n",
    "| **归一化维度** | 对批次维度进行归一化             | 对每个样本的最后一个维度进行归一化   |\n",
    "| **适用场景**   | 适用于固定大小的批次和图像数据   | 适用于变长序列和小批次数据           |\n",
    "| **计算方式**   | 依赖批次的统计信息（均值和方差） | 依赖每个样本的统计信息（均值和方差） |\n",
    "| **可学习参数** | 有可学习的缩放因子和偏移因子     | 有可学习的缩放因子和偏移因子         |\n",
    "| **性能**       | 在批次较大时效果好               | 在批次较小时效果好                   |\n",
    "\n",
    "（4）层规范化的计算示例\n",
    "\n",
    "假设输入张量  x  的形状为  (3, 4) ，表示 3 个样本，每个样本有 4 个特征。\n",
    "\n",
    "#### 输入张量\n",
    "\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "2 & 3 & 4 & 5 \\\\\n",
    "3 & 4 & 5 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### 计算均值和方差\n",
    "\n",
    "- 对每个样本计算均值和方差：\n",
    "\n",
    "  - 样本 1：\n",
    "\n",
    "    均值 \n",
    "    $$\n",
    "    \\mu_1 = \\frac{1+2+3+4}{4} = 2.5\n",
    "    $$\n",
    "    方差 \n",
    "    $$\n",
    "    \\sigma_1^2 = \\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    "  - 样本 2：\n",
    "\n",
    "    均值 \n",
    "    $$\n",
    "    \\mu_2 = \\frac{2+3+4+5}{4} = 3.5\n",
    "    $$\n",
    "    方差 \n",
    "    $$\n",
    "    \\sigma_2^2 = \\frac{(2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    "  - 样本 3：\n",
    "\n",
    "    均值 \n",
    "    $$\n",
    "    \\mu_3 = \\frac{3+4+5+6}{4} = 4.5\n",
    "    $$\n",
    "     方差 \n",
    "    $$\n",
    "    sigma_3^2 = \\frac{(3-4.5)^2 + (4-4.5)^2 + (5-4.5)^2 + (6-4.5)^2}{4} = 1.25\n",
    "    $$\n",
    "     \n",
    "\n",
    " 层规范化的计算结果：\n",
    "\n",
    "  $$\n",
    "  \\text{LayerNorm}(x) = \\begin{bmatrix}\n",
    "  \\frac{1-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{2-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{3-2.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-2.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\frac{2-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{3-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-3.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{5-3.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\frac{3-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{4-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{5-4.5}{\\sqrt{1.25 + \\epsilon}} & \\frac{6-4.5}{\\sqrt{1.25 + \\epsilon}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差连接和层规范化(Add&Norm)\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        \"\"\"\n",
    "        初始化 AddNorm 模块。\n",
    "\n",
    "        参数:\n",
    "        - normalized_shape: 层规范化的输入形状（通常是输入张量的最后一个维度）。\n",
    "        - dropout: Dropout 的概率（用于正则化）。\n",
    "        - **kwargs: 其他传递给父类 nn.Module 的参数。\n",
    "        \"\"\"\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "        - X: 输入张量（通常是上一层的输出）。\n",
    "        - Y: 需要与 X 进行残差连接的张量（通常是当前层的输出）。\n",
    "\n",
    "        返回:\n",
    "        - 经过残差连接和层规范化后的输出张量。\n",
    "        \"\"\"\n",
    "        # 对 Y 应用 Dropout，然后与 X 进行残差连接，最后进行层规范化\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模块实现了 残差连接（Residual Connection） 和 层规范化（LayerNorm） 的功能。\n",
    "\n",
    "残差连接通过将当前层的输出与上一层的输出相加，帮助模型更好地训练深层网络。\n",
    "\n",
    "层规范化对输入进行归一化，使得模型对输入的分布更加稳定。\n",
    "\n",
    "Dropout 用于正则化，防止模型过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.编码器-解码器结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 编码器\n",
    "\n",
    "在编码器接口中，我们只指定长度可变的序列作为编码器的输入X。任何继承这个Encoder基类的模型将完成代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder--decoder architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 解码器\n",
    "\n",
    "在下面的解码器接口中，我们新增一个init_state函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入和编码后的状态映射成当前时间步的输出词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 初始化方法，在这里可以初始化解码器的参数和层\n",
    "        super().__init__()\n",
    "\n",
    "    # init_state方法用于初始化解码器的状态。\n",
    "    # 这个方法应该在每个具体的解码器类中被实现（override）。\n",
    "    # enc_all_outputs参数代表编码器的所有输出，可以用于初始化解码器的状态。\n",
    "    # *args允许传入额外的参数，这提供了灵活性，以适应不同的解码器实现。\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # X代表输入到解码器的序列数据。\n",
    "    # state代表解码器的状态，它可能包含编码器的输出、隐藏状态等信息。\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 编码器-解码器结构\n",
    "\n",
    "编码器‐解码器结构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
