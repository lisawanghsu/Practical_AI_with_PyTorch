{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.多层感知机(感知机：1958年)\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 熟悉线性回归、逻辑回归、Softmax回归、感知机和神经元之间的相互关系\n",
    "\n",
    "2. 熟悉多层感知机模型的构建方法\n",
    "\n",
    "3. 会使用torchvision.transforms.Compose, torchvision.transforms.ToTensor, torchvision.transforms.Normalize\n",
    "\n",
    "4. 会使用torch.utils.data.DataLoader\n",
    "\n",
    "5. 熟悉模型训练和测试的步骤\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机（MLP，Multilayer Perceptron）是深度学习中一种基础的神经网络模型，它由多个层次的神经元组成，包括输入层、一个或多个隐藏层以及输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 感知机\n",
    "\n",
    "感知机是一个非常简单的二元分类器，可以确定给定的输入图像是否属于给定的类。为了实现这一点，它使用了单位阶跃激活函数。使用单位阶跃激活函数，如果输入大于 0，则输出为 1，否则为 0。\n",
    "\n",
    "<img src=\"./images/perceptron.jpg\" style=\"zoom:60%;\" />\n",
    "\n",
    "1958 年，弗兰克 · 罗森布拉特发明了感知机，这是一种非常简单的机器模型，后来成为当今智能机器的核心和起源。罗森布拉特构建的感知机是在名为 Mark I 感知机的硬件中实现的。Mark I 感知机是一台纯电动机器。它有 400 个光电管（或光电探测器），其权重被编码到电位器中，权重更新由电动机执行。\n",
    "\n",
    "<img src=\"./images/MarkI.jpg\" style=\"zoom:60%;\" />\n",
    "\n",
    "感知机的发明在当时引起了极大的关注，甚至登上了《纽约时报》的头条新闻。《纽约时报》的报道中提到，海军期望电子计算机的初步模型能够行走、说话、观察、书写、自我复制并意识到它的存在。这表明感知机在当时被看作是人工智能领域的一个重要突破，尽管它最初的目标仅仅是识别图像中的两个类别。尽管感知机能够处理线性可分问题，但它在处理非线性问题时存在明显局限性。\n",
    "\n",
    "随着时间的推移，人们逐渐认识到通过添加更多的层可以使网络学习更复杂的功能，从而发展出了多层感知机（MLP）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 多层感知机\n",
    "\n",
    "为了解决感知机的局限性，研究者们开始探索包含多个层次的神经网络模型，使网络具有学习复杂功能的能力，产生了多层感知机 (MLP)。多层感知机能够通过非线性激活函数学习复杂的模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 MLP的现代发展\n",
    "\n",
    "尽管多层感知机已经被更复杂的模型如卷积神经网络（CNN）或循环神经网络（RNN）所取代，但MLP依然是深度学习中不可或缺的基础模型，并作为更复杂模型的基础组件所使用。\n",
    "\n",
    "多层感知机作为深度学习的基础，其历史发展是深度学习领域不断探索和创新过程的缩影。通过不断的研究和改进，多层感知机及其衍生模型在人工智能领域中扮演了重要角色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 MLP与线性回归的关系\n",
    "\n",
    "线性回归 + sigmoid函数 = 逻辑回归\n",
    "\n",
    "线性回归 + softmax函数 = softmax回归\n",
    "\n",
    "线性回归 + 阶跃函数    = 感知器 （神经元）\n",
    "\n",
    "感知器级联叠加在一起 = 多层感知器（MLP）（全连接神经网络）\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 基于MLP的手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.导入必需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.数据集的加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Compose 是一个用于图像预处理的函数，它允许你将多个图像转换函数组合成一个序列，这样你就可以方便地对图像数据集应用一系列预处理步骤。\n",
    "\n",
    "transforms.Compose 是torchvision.transforms模块的一部分，经常与数据加载器DataLoader一起使用，以便于在训练或评估神经网络之前对图像进行处理。\n",
    "\n",
    "使用transforms.Compose可以确保图像以一致的方式进行预处理，并且使得预处理步骤的代码更加清晰和易于维护。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.ToTensor\n",
    "\n",
    "PyTorch torchvision.transforms 模块中的一个函数，它用于将 PIL 图像或者 Numpy 数组转换为 torch.FloatTensor 类型的张量。这个转换通常在数据加载和预处理流程中使用，以确保数据以适合神经网络处理的格式提供。\n",
    "\n",
    "使用 transforms.ToTensor 主要有两个目的：\n",
    "\n",
    "数据类型转换：将 PIL 图像的像素值从整数类型转换为浮点数类型，因为 PyTorch 的神经网络层期望输入数据为浮点数。\n",
    "\n",
    "维度调整：将 PIL 图像的维度从 (H, W, C)（高度, 宽度, 通道数）转换为神经网络期望的 (C, H, W)，即通道维在前。\n",
    "\n",
    "转换过程中，像素值会被除以 255.0，将其范围从 [0, 255] 归一化到 [0, 1]。这是因为归一化的数据有助于神经网络更快地收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Normalize\n",
    "\n",
    "PyTorch torchvision.transforms 模块中的一个函数，用于对图像张量进行标准化处理。这种标准化通常在数据预处理流程中使用，目的是将图像的像素值缩放到一个特定的范围，通常是 [-1, 1] 或 [0, 1]，以便神经网络更有效地处理。\n",
    "\n",
    "transforms.Normalize 主要有两个参数：\n",
    "\n",
    "mean：一个数值列表，表示每个通道的均值。在标准化过程中，每个通道的数据会减去对应的均值。\n",
    "\n",
    "std：一个数值列表，表示每个通道的标准差。在标准化过程中，每个通道的数据会除以其标准差。\n",
    "\n",
    "使用 transforms.Normalize 可以确保不同通道的像素值具有相同的尺度，这有助于防止某些通道在训练过程中对损失函数的梯度贡献过大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./datasets', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader是PyTorch中数据读取的一个重要接口，该接口是将自定义的数据集根据batch size、shuffle等的情况封装成Batch Size大小的Tensor，用于后面的训练。我们使用DataLoader来加载训练集和验证集.\n",
    "\n",
    "DataLoader第1个参数dataset表示加载的数据集；\n",
    "\n",
    "第2个参数batch_size表示一次传多少张图片进入模型；\n",
    "\n",
    "第3个参数shuffle表示是否打乱数据集中图像的次序，True代表乱序，False代表次序不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28  # MNIST图像大小为28x28\n",
    "hidden_size = 256   # 隐藏层的大小可以根据需要调整\n",
    "num_classes = 10    # 总共有10个类别（0到9）\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）构建MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train() 是 PyTorch 中的一个方法，用于将模型设置为训练模式。当调用这个方法时，它会改变模型中特定层的行为，以适应训练阶段的需求。model.train() 应该在开始训练循环之前调用，确保模型能够进行训练。\n",
    "\n",
    "在训练模式下，某些层如 Dropout 和 Batch Normalization 会按照它们设计的方式来工作。模型会计算梯度，这是反向传播过程的一部分。这些梯度随后会被优化器用来更新模型的参数。\n",
    "\n",
    "神经网络训练过程中的一个基本循环，通常包含以下步骤：\n",
    "\n",
    "- 前向传播：模型接收输入数据并产生输出。\n",
    "\n",
    "- 计算损失：使用损失函数比较模型输出和真实标签。\n",
    "\n",
    "- 梯度置零：在进行反向传播前，需要将之前累积的梯度清零。\n",
    "\n",
    "- 反向传播：计算损失函数关于模型参数的梯度。\n",
    "\n",
    "- 参数更新：使用优化器根据梯度更新模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.33679\n",
      "Epoch [2/10], Loss: 0.16034\n",
      "Epoch [3/10], Loss: 0.11462\n",
      "Epoch [4/10], Loss: 0.09376\n",
      "Epoch [5/10], Loss: 0.07874\n",
      "Epoch [6/10], Loss: 0.06765\n",
      "Epoch [7/10], Loss: 0.06022\n",
      "Epoch [8/10], Loss: 0.05419\n",
      "Epoch [9/10], Loss: 0.04880\n",
      "Epoch [10/10], Loss: 0.04346\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    total_loss = 0  # 初始化总损失累加器\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "\n",
    "        outputs = model(data.view(-1, input_size))  # 1.前向传播\n",
    "        loss = criterion(outputs, labels)  # 2.计算损失\n",
    "        optimizer.zero_grad()  # 3.清除之前的梯度\n",
    "        loss.backward()  # 4.反向传播，计算当前参数的梯度\n",
    "        optimizer.step()  # 5.更新参数\n",
    "        \n",
    "        total_loss += loss.item()  # 累加损失值\n",
    "\n",
    "    # 计算平均损失并打印\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.eval() 应该放在 with torch.no_grad(): 上下文管理器的外面，因为 model.eval() 改变了模型的状态，而 torch.no_grad() 则是一个上下文管理器，用于临时禁用梯度计算。\n",
    "\n",
    "model.eval() 调用将模型的所有层设置为评估模式，这通常会影响模型中的特定层（如Dropout和Batch Normalization），使它们在评估时的行为与训练时不同。这个状态的改变需要在整个评估过程中保持，而不仅仅是在梯度计算被禁用的期间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试通常是指在训练完成后，使用模型对新的、未见过的数据进行评估，以检验模型的泛化能力。以下是模型测试的一般步骤：\n",
    "\n",
    "- 设置模型为评估模式：\n",
    "\n",
    "使用 model.eval() 将模型设置为评估模式，这会影响模型中特定层的行为，如Dropout和Batch Normalization。\n",
    "\n",
    "- 禁用梯度计算：\n",
    "\n",
    "使用 torch.no_grad() 上下文管理器来禁用梯度计算，这在测试时是必要的，因为它减少了内存消耗并加速了计算过程。\n",
    "\n",
    "- 模型预测：\n",
    "\n",
    "遍历测试数据集，对每个样本进行预测。通常，这涉及到将数据输入模型并获取输出。\n",
    "\n",
    "- 计算性能指标：\n",
    "\n",
    "根据任务类型（如分类、回归等），计算模型的性能指标，例如准确率、精确度、召回率、F1分数、均方误差等。\n",
    "\n",
    "- 分析结果：\n",
    "\n",
    "分析模型在测试集上的表现，确定模型的泛化能力。检查是否存在过拟合或欠拟合的迹象。\n",
    "\n",
    "- 保存模型：\n",
    "\n",
    "如果模型表现良好，可以考虑保存模型的参数，以便将来进行部署或进一步的测试。\n",
    "\n",
    "- 可视化结果（可选）：\n",
    "\n",
    "对于某些任务，如图像分类，可视化一些测试样本及其预测结果可能有助于进一步理解模型的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.63%\n"
     ]
    }
   ],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 使用torch.no_grad()上下文管理器来禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    # 初始化正确预测的数量和总的样本数量\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 遍历测试数据加载器，获取数据和标签\n",
    "    for data, labels in test_loader:\n",
    "        # 调整数据的形状以匹配模型的输入要求,input_size是模型期望的输入特征数量\n",
    "        outputs = model(data.view(-1, input_size))\n",
    "        # print(outputs.shape, data.view(-1, input_size).shape)\n",
    "        # 使用模型的输出来预测类别，并找到预测类别中概率最高的索引\n",
    "        # torch.max返回两个值：最大值和最大值的索引，这里我们使用索引\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # 更新总的样本数量和正确预测的数量\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()  # .item()将tensor转换为数值\n",
    "    # 计算并打印模型的准确率\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
