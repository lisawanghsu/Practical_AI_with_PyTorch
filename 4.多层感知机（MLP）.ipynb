{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.多层感知机(1974年)\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 熟悉线性回归、逻辑回归、Softmax回归、感知机和神经元之间的相互关系\n",
    "\n",
    "2. 熟悉多层感知机模型的构建方法\n",
    "\n",
    "3. 会使用torchvision.transforms.Compose, torchvision.transforms.ToTensor, torchvision.transforms.Normalize\n",
    "\n",
    "4. 会使用torch.utils.data.DataLoader\n",
    "\n",
    "5. 熟悉模型训练和测试的步骤\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机（MLP，Multilayer Perceptron）是基础的神经网络模型，它由多个层次的神经元组成，包括输入层、一个或多个隐藏层以及输出层。多层感知机作为深度学习的基础，其历史发展是人工智能领域不断探索和创新过程的缩影。通过不断的研究和改进，多层感知机及其衍生模型在人工智能领域中扮演了重要角色。\n",
    "\n",
    "尽管多层感知机在很多情况下已经被更复杂的模型如卷积神经网络（CNN）等模型所取代，但MLP依然是深度学习中不可或缺的基础模型，并作为更复杂模型的基础组件所使用。例如在Transformer模型中，其中的核心组件FFN（Feed-Forward Network）就是多层感知机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 感知机\n",
    "\n",
    "感知机是一个非常简单的二元分类器，可以确定给定的输入图像是否属于给定的类。为了实现这一点，它使用了单位阶跃激活函数。使用单位阶跃激活函数，如果输入大于 0，则输出为 1，否则为 0。\n",
    "\n",
    "<img src=\"./images/perceptron.jpg\" style=\"zoom:60%;\" />\n",
    "\n",
    "感知机是一个相当简单的模型，但从感知机到多层感知机的发展历程，却充满了曲折和艰辛。\n",
    "\n",
    "1943年，Warren McCulloch和Walter Pitts发表论文《神经活动内在思想的逻辑演算》，首次提出神经元的M-P模型，并证明可以使用逻辑演算描述神经网络的运行机理，拓展了计算神经科学的领域前沿，奠定了人工神经网络的发展基础。\n",
    "\n",
    "1958 年，弗兰克 · 罗森布拉特发明了感知机，这是一种非常简单的机器模型，后来成为当今智能机器的核心和起源。罗森布拉特构建的感知机是在名为 Mark I 感知机的硬件中实现的。Mark I 感知机是一台纯电动机器。它有 400 个光电管（或光电探测器），其权重被编码到电位器中，权重更新由电动机执行。\n",
    "\n",
    "<img src=\"./images/MarkI.jpg\" style=\"zoom:60%;\" />\n",
    "\n",
    "感知机的发明在当时引起了极大的关注，甚至登上了《纽约时报》的头条新闻。这表明感知机在当时被看作是人工智能领域的一个重要突破，尽管它最初的目标仅仅是识别图像中的两个类别。尽管感知机能够处理线性可分问题，但它在处理非线性问题时存在明显局限性。1969 年，数学家 Marvin Minsky 和 Seymour Parpert 合著了《感知器》一书，从数学的角度证明了单层神经网络的局限性，指出神经网络甚至在面对简单的异或逻辑（XOR）问题时也显得无能为力。导致感知器的研究在接下来的十多年里陷入了停滞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 多层感知机和反向传播算法\n",
    "\n",
    "为了解决感知机的局限性，研究者们开始探索包含多个层次的神经网络模型，使网络具有学习复杂功能的能力。产生了多层感知机 (MLP)。多层感知机能够通过非线性激活函数学习复杂的模式。真正意义上的多层感知机是由 Paul Werbos 提出的，并由 Geoffrey Hinton 等人通过反向传播算法使其成为一种可行的训练方法。\n",
    "\n",
    "1974 年，Paul Werbos 在哈佛大学攻读博士学位期间在其博士论文中首次提出了反向传播（Backpropagation, BP）算法和多层感知机模型，然而他的算法当时并未引起广泛关注和重视。直到1986年，David Rumelhart, Geoffrey Hinton和 Ronald Williams在Nature期刊上发表了一篇名为《通过反向误差来学习》的论文，将反向传播算法应用于MLP并且证明了这种算法对多层神经网络训练的有效性，Hinton 等人的工作对多层感知机的推广和应用起到了关键作用。同年，Rumelhart 和 McClelland 编辑的知名著作《并行分布式处理：认知的微结构中的探索》发表。这本书在反向传播的应用上产生了重大影响，使得反向传播在多层感知器的训练上成为了最流行的学习算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 反向传播和梯度下降的关系\n",
    "\n",
    "反向传播算法（Backpropagation, BP） 和 梯度下降法（Gradient Descent, GD） 是深度学习中两个密切相关的概念，它们共同构成了训练神经网络的核心方法。\n",
    "\n",
    "梯度下降法是一种优化算法，用于最小化损失函数。它通过计算目标函数的梯度（即函数对参数的偏导数），并沿着梯度的反方向更新参数，从而逐步逼近最小值。而反向传播算法通过链式法则（Chain Rule）将输出层的误差逐层反向传播到输入层，从而高效地计算每一层参数的梯度。梯度下降法需要知道损失函数对每个参数的梯度，而反向传播算法提供了计算这些梯度的具体方法。如果没有反向传播算法，梯度下降法无法高效地应用于多层神经网络。\n",
    "\n",
    "因此，如果没有反向传播算法，梯度下降法无法高效地应用于多层神经网络；而如果没有梯度下降法，反向传播计算出的梯度也无法用于优化模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 从线性回归到MLP\n",
    "\n",
    "线性回归 + sigmoid函数 = 逻辑回归\n",
    "\n",
    "线性回归 + softmax函数 = softmax回归\n",
    "\n",
    "线性回归 + 阶跃函数    = 感知机 （神经元）\n",
    "\n",
    "感知机级联叠加在一起 = 多层感知机（MLP）（全连接神经网络）\n",
    "\n",
    "### 4.5 MNIST数据集\n",
    "\n",
    "MNIST数据集起源于美国国家标准与技术研究所（National Institute of Standards and Technology，简称NIST）。该数据集最初由NIST收集整理，由Yann LeCun等人在20世纪90年代创建，目的是通过算法实现对手写数字的识别。MNIST数据集是深度学习和图像识别研究中的一个基准数据集，包含了来自250个不同人的手写数字图片，其中一半是高中生的手写样本，另一半则来自人口普查局的工作人员。它包含了60,000个训练样本和10,000个测试样本，每个样本是一个28x28像素的灰度图像，代表从0到9的手写数字。\n",
    "\n",
    "下面，我们就用MNIST数据集来训练一个MLP模型，实现手写数字的识别。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 MLP的应用——手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.导入必需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.数据集的加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Compose 是一个用于图像预处理的函数，它允许你将多个图像转换函数组合成一个序列，这样你就可以方便地对图像数据集应用一系列预处理步骤。\n",
    "\n",
    "transforms.Compose 是torchvision.transforms模块的一部分，经常与数据加载器DataLoader一起使用，以便于在训练或评估神经网络之前对图像进行处理。\n",
    "\n",
    "使用transforms.Compose可以确保图像以一致的方式进行预处理，并且使得预处理步骤的代码更加清晰和易于维护。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.ToTensor\n",
    "\n",
    "PyTorch torchvision.transforms 模块中的一个函数，它用于将 PIL 图像或者 Numpy 数组转换为 torch.FloatTensor 类型的张量。这个转换通常在数据加载和预处理流程中使用，以确保数据以适合神经网络处理的格式提供。\n",
    "\n",
    "使用 transforms.ToTensor 主要有两个目的：\n",
    "\n",
    "数据类型转换：将 PIL 图像的像素值从整数类型转换为浮点数类型，因为 PyTorch 的神经网络层期望输入数据为浮点数。\n",
    "\n",
    "维度调整：将 PIL 图像的维度从 (H, W, C)（高度, 宽度, 通道数）转换为神经网络期望的 (C, H, W)，即通道维在前。\n",
    "\n",
    "转换过程中，像素值会被除以 255.0，将其范围从 [0, 255] 归一化到 [0, 1]。这是因为归一化的数据有助于神经网络更快地收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Normalize\n",
    "\n",
    "PyTorch torchvision.transforms 模块中的一个函数，用于对图像张量进行标准化处理。这种标准化通常在数据预处理流程中使用，目的是将图像的像素值缩放到一个特定的范围，通常是 [-1, 1] 或 [0, 1]，以便神经网络更有效地处理。\n",
    "\n",
    "transforms.Normalize 主要有两个参数：\n",
    "\n",
    "mean：一个数值列表，表示每个通道的均值。在标准化过程中，每个通道的数据会减去对应的均值。\n",
    "\n",
    "std：一个数值列表，表示每个通道的标准差。在标准化过程中，每个通道的数据会除以其标准差。\n",
    "\n",
    "使用 transforms.Normalize 可以确保不同通道的像素值具有相同的尺度，这有助于防止某些通道在训练过程中对损失函数的梯度贡献过大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./datasets', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader是PyTorch中数据读取的一个重要接口，该接口是将自定义的数据集根据batch size、shuffle等的情况封装成Batch Size大小的Tensor，用于后面的训练。我们使用DataLoader来加载训练集和验证集.\n",
    "\n",
    "DataLoader第1个参数dataset表示加载的数据集；\n",
    "\n",
    "第2个参数batch_size表示一次传多少张图片进入模型；\n",
    "\n",
    "第3个参数shuffle表示是否打乱数据集中图像的次序，True代表乱序，False代表次序不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28  # MNIST图像大小为28x28\n",
    "hidden_size = 256   # 隐藏层的大小可以根据需要调整\n",
    "num_classes = 10    # 总共有10个类别（0到9）\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）构建MLP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.train() 是 PyTorch 中的一个方法，用于将模型设置为训练模式。当调用这个方法时，它会改变模型中特定层的行为，以适应训练阶段的需求。model.train() 应该在开始训练循环之前调用，确保模型能够进行训练。\n",
    "\n",
    "在训练模式下，某些层如 Dropout 和 Batch Normalization 会按照它们设计的方式来工作。模型会计算梯度，这是反向传播过程的一部分。这些梯度随后会被优化器用来更新模型的参数。\n",
    "\n",
    "神经网络训练过程中的一个基本循环，通常包含以下步骤：\n",
    "\n",
    "- 前向传播：模型接收输入数据并产生输出。\n",
    "\n",
    "- 计算损失：使用损失函数比较模型输出和真实标签。\n",
    "\n",
    "- 梯度置零：在进行反向传播前，需要将之前累积的梯度清零。\n",
    "\n",
    "- 反向传播：计算损失函数关于模型参数的梯度。\n",
    "\n",
    "- 参数更新：使用优化器根据梯度更新模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.33679\n",
      "Epoch [2/10], Loss: 0.16034\n",
      "Epoch [3/10], Loss: 0.11462\n",
      "Epoch [4/10], Loss: 0.09376\n",
      "Epoch [5/10], Loss: 0.07874\n",
      "Epoch [6/10], Loss: 0.06765\n",
      "Epoch [7/10], Loss: 0.06022\n",
      "Epoch [8/10], Loss: 0.05419\n",
      "Epoch [9/10], Loss: 0.04880\n",
      "Epoch [10/10], Loss: 0.04346\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    total_loss = 0  # 初始化总损失累加器\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "\n",
    "        outputs = model(data.view(-1, input_size))  # 1.前向传播\n",
    "        loss = criterion(outputs, labels)  # 2.计算损失\n",
    "        optimizer.zero_grad()  # 3.清除之前的梯度\n",
    "        loss.backward()  # 4.反向传播，计算当前参数的梯度\n",
    "        optimizer.step()  # 5.更新参数\n",
    "        \n",
    "        total_loss += loss.item()  # 累加损失值\n",
    "\n",
    "    # 计算平均损失并打印\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.模型测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "model.eval() 应该放在 with torch.no_grad(): 上下文管理器的外面，因为 model.eval() 改变了模型的状态，而 torch.no_grad() 则是一个上下文管理器，用于临时禁用梯度计算。\n",
    "\n",
    "model.eval() 调用将模型的所有层设置为评估模式，这通常会影响模型中的特定层（如Dropout和Batch Normalization），使它们在评估时的行为与训练时不同。这个状态的改变需要在整个评估过程中保持，而不仅仅是在梯度计算被禁用的期间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试通常是指在训练完成后，使用模型对新的、未见过的数据进行评估，以检验模型的泛化能力。以下是模型测试的一般步骤：\n",
    "\n",
    "- 设置模型为评估模式：\n",
    "\n",
    "使用 model.eval() 将模型设置为评估模式，这会影响模型中特定层的行为，如Dropout和Batch Normalization。\n",
    "\n",
    "- 禁用梯度计算：\n",
    "\n",
    "使用 torch.no_grad() 上下文管理器来禁用梯度计算，这在测试时是必要的，因为它减少了内存消耗并加速了计算过程。\n",
    "\n",
    "- 模型预测：\n",
    "\n",
    "遍历测试数据集，对每个样本进行预测。通常，这涉及到将数据输入模型并获取输出。\n",
    "\n",
    "- 计算性能指标：\n",
    "\n",
    "根据任务类型（如分类、回归等），计算模型的性能指标，例如准确率、精确度、召回率、F1分数、均方误差等。\n",
    "\n",
    "- 分析结果：\n",
    "\n",
    "分析模型在测试集上的表现，确定模型的泛化能力。检查是否存在过拟合或欠拟合的迹象。\n",
    "\n",
    "- 保存模型：\n",
    "\n",
    "如果模型表现良好，可以考虑保存模型的参数，以便将来进行部署或进一步的测试。\n",
    "\n",
    "- 可视化结果（可选）：\n",
    "\n",
    "对于某些任务，如图像分类，可视化一些测试样本及其预测结果可能有助于进一步理解模型的行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.63%\n"
     ]
    }
   ],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 使用torch.no_grad()上下文管理器来禁用梯度计算\n",
    "with torch.no_grad():\n",
    "    # 初始化正确预测的数量和总的样本数量\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 遍历测试数据加载器，获取数据和标签\n",
    "    for data, labels in test_loader:\n",
    "        # 调整数据的形状以匹配模型的输入要求,input_size是模型期望的输入特征数量\n",
    "        outputs = model(data.view(-1, input_size))\n",
    "        # print(outputs.shape, data.view(-1, input_size).shape)\n",
    "        # 使用模型的输出来预测类别，并找到预测类别中概率最高的索引\n",
    "        # torch.max返回两个值：最大值和最大值的索引，这里我们使用索引\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # 更新总的样本数量和正确预测的数量\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()  # .item()将tensor转换为数值\n",
    "    # 计算并打印模型的准确率\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
