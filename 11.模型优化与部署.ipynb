{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.模型优化与部署\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 了解模型剪枝的原理和Torch-Pruning结构化剪枝框架\n",
    "\n",
    "2. 会使用Torch-Pruning对常见的模型进行结构化剪枝\n",
    "\n",
    "3. 了解模型量化的原理和Quanto量化工具\n",
    "\n",
    "4. 熟悉模型量化的基本流程，能够对常见模型进行量化\n",
    "\n",
    "5. 了解模型在工业现场的部署方式\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型优化是指在机器学习和深度学习中，通过各种技术手段改善模型的性能、效率和可部署性的过程。这通常包括以下几个方面：\n",
    "\n",
    "- 加速推理速度：通过减少模型的计算复杂度，提高模型在推理时的速度，使其能够更快地响应请求。\n",
    "\n",
    "- 减少模型大小：通过剪枝、量化、权重共享等技术减少模型的存储大小，便于模型的传输和存储。\n",
    "\n",
    "- 跨平台兼容性：使模型能够在不同的硬件和软件平台上运行，如CPU、GPU、TPU、移动设备等。\n",
    "\n",
    "- 适应特定的部署环境：根据模型部署的目标环境（如云端、边缘设备、嵌入式系统等），进行特定的优化。\n",
    "\n",
    "模型优化的具体技术包括：模型剪枝、模型量化、模型蒸馏、混合精度训练等。模型优化的目标是在不牺牲太多模型性能的前提下，使模型更适合实际应用的需求。\n",
    "\n",
    "本节主要讲解模型剪枝和模型量化两种优化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工智能模型的剪枝和量化是两种重要的模型优化技术，它们对于大模型和边缘设备的部署尤为关键。\n",
    "\n",
    "**对于大模型而言**，随着深度学习技术的飞速发展，模型复杂度和参数量急剧增加，这给模型的训练和部署带来了巨大挑战。模型剪枝通过移除不重要的参数或结构来减小模型大小和复杂度，从而提高推理速度和降低计算资源消耗。量化技术通过降低模型参数和中间数据的精度来减小模型大小和计算量，常见的量化方法包括固定大小量化和自动调整区间量化。\n",
    "\n",
    "**在边缘设备部署方面**，边缘设备通常具有有限的计算能力和内存，并且需要在低功耗下运行。模型剪枝可以高效生成规模更小、内存利用率更高、能耗更低的模型，这对于在移动设备上运行它们非常重要。量化可以显著减少模型的存储需求和计算复杂度，使得深度学习模型能够在资源受限的边缘设备上运行。\n",
    "\n",
    "此外，模型剪枝和量化还可以提高模型的部署效率和响应速度，降低模型的总体拥有成本（TCO），并且在数据隐私和安全性方面提供优势，因为数据可以在当地处理而不需要发送到云端。\n",
    "\n",
    "因此，剪枝和量化技术对于大模型的优化和边缘设备的部署至关重要，它们使得AI模型更加高效、轻便，能够满足不同应用场景的需求。随着这些技术的不断发展和创新，未来边缘设备上部署大模型将变得更加可行和高效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 模型剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **什么是模型剪枝？**\n",
    "\n",
    "模型剪枝（Model Pruning）是一种模型压缩技术，它通过移除神经网络中的一些权重或神经元来减少模型的复杂度和大小。这样做的目的是为了提高模型的推理速度，降低模型对计算资源的需求，同时尽量保持模型的性能（如准确率）。模型剪枝通常在模型训练完成后进行。\n",
    "\n",
    "模型剪枝可以分为两大类：\n",
    "\n",
    "**非结构化剪枝（Unstructured Pruning）**：\n",
    "   - 在这种剪枝中，网络中的单个权重被设置为零（即被“剪掉”），而不是整个神经元或通道。\n",
    "   - 这种方法不改变模型的架构，但是可能会导致计算图变得稀疏。\n",
    "\n",
    "**结构化剪枝（Structured Pruning）**：\n",
    "   - 结构化剪枝涉及移除整个神经元或过滤器（例如，卷积神经网络中的整个卷积核）。\n",
    "   - 这种方法会改变模型的结构，但是可以更显著地减少模型的计算复杂度。\n",
    "\n",
    "剪枝过程通常包括以下步骤：\n",
    "\n",
    "- **选择剪枝策略**：基于权重的大小、梯度、激活或其他指标来确定哪些权重或神经元是“不重要”的。\n",
    "- **剪枝实施**：将选定的权重或神经元设置为零或从模型中移除。\n",
    "- **微调**：剪枝后，模型通常需要在训练数据上进行额外的训练（微调），以恢复由于剪枝而可能损失的性能。\n",
    "\n",
    "模型剪枝的优点包括减少模型大小、加快推理速度、降低能耗，以及在某些情况下提高模型的泛化能力。然而，剪枝也可能导致模型精度的下降，特别是在过度剪枝的情况下。因此，剪枝需要谨慎进行，以平衡模型大小和性能。\n",
    "\n",
    "\n",
    "2. **为什么需要模型剪枝？**\n",
    "\n",
    "进行模型剪枝主要出于以下几个原因：\n",
    "\n",
    " **减少模型大小**：剪枝可以显著减少模型的参数量，生成更小的模型文件，这有助于降低存储需求。\n",
    "\n",
    " **加快推理速度**：通过减少模型的参数和计算量，剪枝可以加快模型的推理速度，使模型能够在资源受限的环境中更快地做出预测。\n",
    "\n",
    " **降低能耗**：在移动设备或边缘设备上，减少计算量可以显著降低模型运行时的能耗，这对于电池供电的设备尤为重要。\n",
    "\n",
    " **提高硬件利用率**：剪枝后的模型由于计算需求降低，可以更高效地利用硬件资源，如GPU或TPU，从而提升整体的系统性能。\n",
    "\n",
    " **改善模型泛化能力**：剪枝有时可以去除模型中的冗余参数，这些参数可能会导致过拟合。通过剪枝，模型可能会变得更加简洁，从而提高其泛化能力。\n",
    "\n",
    " **适应不同的部署环境**：不同的部署环境对模型大小和计算能力有不同的限制。剪枝可以帮助模型适应这些限制，实现在多种环境下的部署。\n",
    "\n",
    " **减少内存占用**：在内存受限的设备上，较小的模型可以减少内存占用，避免内存溢出的问题。\n",
    "\n",
    " **降低训练成本**：对于需要频繁更新的模型，剪枝可以减少重新训练时所需的计算资源和时间。\n",
    "\n",
    " **提升部署灵活性**：剪枝后的模型由于体积更小，可以更容易地部署到不同的平台和设备上，增加模型的应用范围。\n",
    "\n",
    "总之，模型剪枝是为了在保持模型性能的同时，提高模型的效率和实用性，使其更适合实际应用的需求。\n",
    "\n",
    "3. **模型剪枝的大乐透假设**\n",
    "\n",
    "​\t\t在实践过程中我们可以感受到大的网络比小的网络更容易训练，而且也有越来越多的实验证明大的网络比小的网络更容易收敛到全局最优点而不会遇到局部最优点和鞍点的问题。解释这一想象的一个假设是**大乐透假设（Lottery Ticket Hypothesis）**。\n",
    "\n",
    "​\t\t在下图中，首先我们使用一个大的网络然后随机初始化一组参数，这组参数用红色表示，然后训练后得到紫色的参数，接着进行网络剪枝。我们再尝试使用剪枝的网络结构随机初始化一组参数然后训练发现这种方式没能取得剪枝得到的效果，而如果用大的网络中对应的初始化参数来初始化这个剪枝的网络结构然后再进行训练，就发现可以取得较好的效果。\n",
    "\n",
    "<img src=\"./images/prune-x.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "​\t\t大乐透假设可以用来解释这个现象，在买大乐透时买得越多就越容易中奖，同样的这里我们假设一个大的网络中包含很多小的网络，这些小的网络结构有的可以训练成功而有的不可以训练成功，只要有一个训练成功，整个大的网络结构就可以训练成功，因此我们可以把多余的网络结构剪枝掉。\n",
    "\n",
    "4. **模型结构化剪枝框架--Torch-Pruning**\n",
    "\n",
    "2013年CVPR提出的非深度图算法DepGraph实现了架构通用的结构化剪枝。DepGraph能够自动地分析复杂的结构耦合，从而正确地移除参数实现网络加速。Torch-Pruning是一个基于DepGraph算法开发的PyTorch结构化剪枝框架。它能够实际地移除参数和通道，降低模型推理成本。在DepGraph的帮助下，研究者和工程师无需再与复杂的网络结构斗智斗勇，可以轻松完成复杂模型的一键剪枝。\n",
    "\n",
    "Torch-Pruning支持对各种深度神经网络进行结构化剪枝，包括 大语言模型（LLMs）、Segment Anything Model（SAM）、扩散模型、Yolo模型、视觉Transformer、Swin Transformer、BERT、FasterRCNN、SSD、ResNe(X)t、ConvNext、DenseNet、RegNet、DeepLab等。不同于 torch.nn.utils.prune 通过参数置零参实现的剪枝，Torch-Pruning 使用一种名为 DepGraph 的算法物理移除互相耦合的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. DepGraph的关键概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依赖图（DepGraph）是Torch Prunning的核心功能，提供了一种自动对依赖层进行分组的机制，它有几个关键概念：\n",
    "\n",
    "tp.dependency.Dependency: the dependency between layers.\n",
    "tp.dependency.DependencyGraph: A relational graph to model the dependency.\n",
    "tp.dependency.Group: A list of dependencies that represents the minimally-removable units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）获取组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = DG.get_pruning_group( model.conv1, tp.prune_conv_out_channels, idxs=[2, 6, 9] )\n",
    "                   # Choose  1. a root layer, 2. a pruning function,   3. a index list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）打印组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(group.details()) # use print(group) if you are not interested in the full idxs list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）修剪组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the pre-defined idxs:\n",
    "print(group[0].idxs)\n",
    "group.prune()\n",
    "\n",
    "# with new idxs:\n",
    "new_idxs = [1,2,3,4]\n",
    "group.prune(new_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）遍历组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (dep, idxs) in enumerate(group):\n",
    "    print(\"Dep: \", dep, \" Idxs:\", idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（5）获取层和剪枝函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (dep, idxs) in enumerate(group):\n",
    "    layer = dep.layer\n",
    "    pruning_fn = dep.pruning_fn\n",
    "    print(layer, pruning_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（6）扫描所有组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in DG.get_all_groups():\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruner.step 返回 group或者None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 ResNet18模型的结构化剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch_pruning as tp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_category = 100\n",
    "\n",
    "# 加载预训练模型\n",
    "model = resnet18(pretrained=True)\n",
    "model.load_state_dict(torch.load('./weights/resnet18_ft.pth'))\n",
    "model.fc.out_features = num_category  # 调整全连接层的输出数目\n",
    "model = model.to(device)\n",
    "\n",
    "# 准备一个随机初始化的输入张量，以匹配模型的输入尺寸\n",
    "example_inputs = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# 构建DepGraph实例，并使用模型和输入张量来分析依赖关系\n",
    "DG = tp.DependencyGraph().build_dependency(model, example_inputs=example_inputs)\n",
    "\n",
    "# 定义重要性评估指标，这里使用L2范数（Magnitude Importance）\n",
    "imp = tp.importance.MagnitudeImportance(p=2)\n",
    "\n",
    "# 忽略不需要剪枝的层，例如最后的全连接层\n",
    "ignored_layers = [model.fc]\n",
    "\n",
    "# 初始化剪枝器，这里使用MetaPruner\n",
    "pruner = tp.pruner.MetaPruner(\n",
    "    model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    pruning_ratio=0.5,  # 设置剪枝比例为50%\n",
    "    ignored_layers=ignored_layers,\n",
    "    global_pruning=True  # 如果需要，可以启用全局剪枝\n",
    ")\n",
    "\n",
    "# 执行剪枝步骤\n",
    "pruner.step()\n",
    "\n",
    "# 剪枝后，保存模型\n",
    "torch.save(model, './weights/pruned_resnet18.pth')\n",
    "\n",
    "# 如果需要，对剪枝后的模型进行微调\n",
    "# 这里需要定义微调的损失函数、优化器和数据加载器\n",
    "# ...\n",
    "\n",
    "# # 加载剪枝后的模型进行评估或进一步训练\n",
    "# model = torch.load('./weights/pruned_resnet18.pth')\n",
    "# model.eval()  # 或 model.train() 如果进行微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用剪枝过的模型进行推理，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "data_path = './datasets/butterflies and moths/valid'\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    ])\n",
    "\n",
    "# （1）加载模型\n",
    "model = torch.load('./weights/pruned_resnet18.pth')\n",
    "model = model.to(device)  # (4) 设备转移（模型）\n",
    "\n",
    "# （7）类别标签\n",
    "label_cls = [folder for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))]\n",
    "\n",
    "# （2）预处理数据\n",
    "image = cv2.imread(\"./datasets/butterflies and moths/test/ADONIS/2.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "img = Image.fromarray(image)\n",
    "img = val_transform(img).unsqueeze(0).to(device)  # (3) 添加批次维度  (4) 设备转移（数据）\n",
    "\n",
    "# （5）模型推理\n",
    "model.eval()\n",
    "output = model(img)\n",
    "\n",
    "# （6）后处理\n",
    "predicted_index = torch.argmax(output, dim=1).cpu().item()  # 获取预测的类别索引\n",
    "\n",
    "# （7）映射到类别标签  （8）输出结果\n",
    "print(f\"识别结果: {label_cls[predicted_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 模型量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 基于Quanto的ResNet18模型量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 特点\n",
    "Quanto作为一个多功能的PyTorch量化工具包，具备以下显著特性：\n",
    "\n",
    "多设备支持：生成的量化模型可以运行于任何设备（包括CUDA设备和MPS设备）上。\n",
    "无缝量化工作流：自动插入量化和反量化结点，支持静态量化和动态量化等多种量化方案。\n",
    "灵活的数据类型支持：不仅支持int8权重和激活，还支持int2、int4以及float8等数据类型。\n",
    "易用性：旨在降低量化技术的使用门槛，使开发者能够轻松实现新算法或组合现有模块。\n",
    "\n",
    "Quanto可以通过简单的API调用，将PyTorch模型转换为量化模型。例如，使用quanto.quantize函数可以直接对模型进行量化，同时指定权重和激活的数据类型。\n",
    "\n",
    "2. 量化流程【代码待测试与修改】\n",
    "\n",
    "Quanto的量化流程大致可以分为以下几个步骤：\n",
    "\n",
    "模型准备：将PyTorch模型准备好，确保模型已经训练完成并处于评估模式。\n",
    "量化配置：通过QuantoConfig设置量化参数，如权重和激活的数据类型。\n",
    "量化模型：使用quanto.quantize函数对模型进行量化。\n",
    "模型评估：评估量化后模型的性能，必要时进行微调以恢复性能。\n",
    "部署与应用：将量化后的模型部署到目标设备上，并进行实际应用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "#from quanto.quantize import Quantizer\n",
    "from quanto import Quantizer\n",
    "from quanto.export import prepare_model\n",
    "\n",
    "# 加载预训练的 ResNet18 模型\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 定义量化配置\n",
    "quantizer = Quantizer(\n",
    "    model,\n",
    "    quantization_config={\n",
    "        'n_bits': 8,  # 量化位宽\n",
    "        'quantization_type': 'symmetric',  # 量化类型\n",
    "        'per_channel': False,  # 是否按通道量化\n",
    "        'quantize_first_and_last_layer': True,  # 是否量化第一层和最后一层\n",
    "    },\n",
    "    calibration_config={\n",
    "        'calibration_method': 'percentile',  # 校准方法\n",
    "        'percentile': 99.99,  # 百分位数\n",
    "    }\n",
    ")\n",
    "\n",
    "# 准备模型进行量化\n",
    "quantized_model = prepare_model(quantizer)\n",
    "\n",
    "# 保存量化后的模型\n",
    "torch.save(quantized_model.state_dict(), 'quantized_resnet18.pth')\n",
    "\n",
    "# 加载量化后的模型进行推理\n",
    "model.load_state_dict(torch.load('quantized_resnet18.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 准备输入数据\n",
    "input_data = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# 执行推理\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 模型部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际工业中部署模型通常涉及以下几个关键步骤：\n",
    "\n",
    "模型转换与优化：首先需要将训练好的模型转换为适合部署的格式。例如，PyTorch模型可以通过TorchScript或ONNX转换，而TensorFlow模型可以直接使用SavedModel格式或转换为TensorFlow Lite。转换后，模型可能需要进一步优化，如量化、剪枝、蒸馏等，以提高推理速度和减少模型大小。\n",
    "\n",
    "模型服务部署：模型转换和优化完成后，需要将其部署为提供预测服务的形式。这通常涉及到使用Web框架（如Flask、Django）构建API服务，或者使用专门的模型服务框架（如TorchServe、TensorFlow Serving）。在一些情况下，可能会使用容器化技术（如Docker）和编排工具（如Kubernetes）来实现大规模分布式部署。\n",
    "\n",
    "性能优化：部署后，还需要对模型服务进行性能优化，包括硬件加速（利用GPU、TPU等）、批处理、缓存优化和分布式部署等。\n",
    "\n",
    "监控与运维：建立监控系统来跟踪模型服务的性能指标，使用日志分析系统来及时发现和定位问题，并确保模型服务的高可用性。\n",
    "\n",
    "安全性保护：在部署过程中，需要考虑数据和模型的安全性，包括使用HTTPS、身份验证和权限管理等措施。\n",
    "\n",
    "A/B测试：在实际部署中，可能会使用A/B测试来评估新模型的效果，并确保平稳过渡。\n",
    "\n",
    "在具体技术选型上，可能会使用TensorRT、LibTorch、OpenVINO等工具来加速模型的推理。例如，TensorRT是NVIDIA提供的一个用于深度学习推理优化的工具，它可以显著提升GPU上的推理性能 \n",
    "。\n",
    "\n",
    "此外，还有一些开源项目和平台，如BentoML、ModelOps等，提供了模型全生命周期管理的工具，帮助企业更高效地进行模型的研发、部署和管理 \n",
    "。\n",
    "\n",
    "在部署过程中，还需要考虑模型的版本控制、API流水线、监控分析等，以确保模型的持续迭代和优化 \n",
    "。\n",
    "\n",
    "总的来说，深度学习模型的工业部署是一个涉及多个方面的复杂工程问题，需要根据具体的业务需求、性能要求和基础设施情况来选择合适的部署方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PT转ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练好的模型通常都需要转化为中间件ONNX格式。ONNX（Open Neural Network Exchange）是一个开放的格式，它允许模型在不同的深度学习框架之间进行转换和使用。转换模型为ONNX格式通常涉及以下步骤：\n",
    "\n",
    "（1）确保模型已经在PyTorch中被量化。\n",
    "\n",
    "（2）将模型设置为评估模式（model.eval()）。\n",
    "\n",
    "（3）准备一个模型输入的示例（dummy input），这个输入将被用于导出模型的结构和形状信息。\n",
    "\n",
    "（4）使用torch.onnx.export函数将模型导出为ONNX格式。\n",
    "\n",
    "（工业界也有另外一种处理流程，即先将模型转化为ONNX格式，之后再对模型进行剪枝和量化等优化操作）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# 假设 `quantized_model` 是已经被Quanto量化的模型\n",
    "quantized_model.eval()\n",
    "\n",
    "# 准备一个模型输入的示例\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# 指定ONNX文件的保存路径\n",
    "onnx_file_name = \"quantized_model.onnx\"\n",
    "\n",
    "# 导出模型\n",
    "torch.onnx.export(quantized_model,                # 被量化的模型\n",
    "                  dummy_input,                  # 模型输入的示例\n",
    "                  onnx_file_name,               # ONNX文件的保存路径\n",
    "                  export_params=True,           # 是否导出模型参数\n",
    "                  opset_version=10,             # ONNX算子集的版本\n",
    "                  do_constant_folding=True,    # 是否执行常量折叠优化\n",
    "                  input_names=['input'],       # 输入模型的张量的名称\n",
    "                  output_names=['output'],     # 输出模型的张量的名称\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # 可变长度的轴\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. C++推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将模型转换为ONNX格式后，可以使用ONNX Runtime进行C++推理。ONNX Runtime是一个高性能的推理引擎，支持多种硬件平台，包括CPU和GPU。\n",
    "\n",
    "以下是一个简单的示例，展示了如何使用ONNX Runtime C++ API进行模型推理：\n",
    "\n",
    "准备环境：\n",
    "\n",
    "首先，确保你已经安装了ONNX Runtime C++库。你可以从ONNX Runtime的官方GitHub仓库下载并构建库。\n",
    "编写C++推理代码：\n",
    "\n",
    "以下是一个基本的C++代码示例，展示了如何加载ONNX模型并进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <onnxruntime/core/session/onnxruntime_cxx_api.h>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "int main() {\n",
    "    // Initialize the ONNX Runtime environment.\n",
    "    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, \"test\");\n",
    "\n",
    "    // Specify the model path.\n",
    "    const char* model_path = \"model.onnx\";\n",
    "\n",
    "    // Create an ONNX Runtime session options object.\n",
    "    Ort::SessionOptions session_options;\n",
    "    session_options.SetIntraOpNumThreads(1);\n",
    "\n",
    "    // Create the session.\n",
    "    Ort::Session session(env, model_path, session_options);\n",
    "\n",
    "    // Get the input layer name.\n",
    "    Ort::AllocatorWithDefaultOptions allocator;\n",
    "    size_t num_input_nodes = session.GetInputCount();\n",
    "    std::vector<const char*> input_node_names;\n",
    "\n",
    "    for (size_t i = 0; i < num_input_nodes; i++) {\n",
    "        char* input_name = session.GetInputName(i, allocator);\n",
    "        input_node_names.push_back(input_name);\n",
    "        // Release the memory allocated for the name.\n",
    "        allocator.Free(input_name);\n",
    "    }\n",
    "\n",
    "    // Prepare the input data.\n",
    "    std::vector<float> input_data = { ... }; // Replace with your input data.\n",
    "    std::vector<int64_t> input_dims = { 1, 3, 224, 224 }; // Replace with your input dimensions.\n",
    "\n",
    "    // Create input tensor object.\n",
    "    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(allocator, input_data.data(), input_data.size(), input_dims.data(), input_dims.size());\n",
    "\n",
    "    // Run the model.\n",
    "    auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), &input_tensor, 1, /*output_node_names_data=*/nullptr, /*num_outputs=*/0);\n",
    "\n",
    "    // Process the output tensor.\n",
    "    float* floatarr = output_tensors.front().GetTensorMutableData<float>();\n",
    "    // Do something with the output data.\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 模型SDK的制作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练完成的模型打包成SDK（软件开发工具包）是一个涉及多个步骤的过程，旨在使模型易于在不同应用程序中集成和使用。以下是将模型打包成SDK的一般步骤：\n",
    "\n",
    "1. **模型优化和转换**：\n",
    "   - 对模型进行必要的优化，如剪枝、量化、转换为低精度格式等，以提高模型的运行效率。\n",
    "   - 如果模型是使用PyTorch、TensorFlow等框架训练的，将其转换为一种更适合部署的格式，如ONNX、TorchScript、TensorFlow Lite等。\n",
    "\n",
    "2. **选择推理引擎**：\n",
    "   - 根据目标平台选择合适的推理引擎，如ONNX Runtime、TensorRT、Core ML、TensorFlow Lite等。\n",
    "\n",
    "3. **编写推理代码**：\n",
    "   - 使用选定的推理引擎，编写加载模型、处理输入输出数据、执行模型推理的C++代码。\n",
    "\n",
    "4. **封装API**：\n",
    "   - 将推理代码封装成一套API，提供简洁的接口供外部调用。这些API可以是函数、类或库。\n",
    "\n",
    "5. **创建示例和文档**：\n",
    "   - 提供使用SDK的示例代码，展示如何加载模型、进行推理等。\n",
    "   - 编写详细的开发文档，包括API说明、使用指南、故障排除等。\n",
    "\n",
    "6. **编译和构建**：\n",
    "   - 将代码编译成库文件（如`.dll`、`.so`、`.dylib`等），并确保所有依赖项都已正确链接。\n",
    "   - 创建安装包或分发包，包含库文件、头文件、许可证文件和示例代码。\n",
    "\n",
    "7. **测试**：\n",
    "   - 在目标平台上对SDK进行彻底的测试，确保其稳定性和性能符合预期。\n",
    "\n",
    "8. **打包和分发**：\n",
    "   - 将编译后的库、头文件、示例代码、文档等打包，可以通过压缩文件、安装程序或其他分发方法提供给用户。\n",
    "\n",
    "9. **版本控制和更新**：\n",
    "   - 为SDK设置版本号，便于管理和更新。\n",
    "   - 根据用户反馈和新的需求，定期更新SDK。\n",
    "\n",
    "10. **许可和版权**：\n",
    "    - 确保所有使用的第三方库和代码都符合相应的许可协议。\n",
    "    - 在SDK中包含必要的版权和许可信息。\n",
    "\n",
    "11. **支持和维护**：\n",
    "    - 提供技术支持和维护服务，帮助用户解决使用SDK时遇到的问题。\n",
    "\n",
    "在实际操作中，你可能还需要考虑以下因素：\n",
    "\n",
    "- **平台兼容性**：确保SDK能够在不同的操作系统和硬件平台上运行。\n",
    "- **安全性**：确保SDK的安全性，特别是如果它将被用于生产环境。\n",
    "- **国际化**：如果SDK将被全球用户使用，考虑支持多语言。\n",
    "- **定制化**：根据目标用户群体的需求，提供定制化的SDK版本。\n",
    "\n",
    "打包SDK是一个涉及多个领域的综合任务，需要软件工程、系统架构、项目管理和用户体验设计等多方面的知识。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
