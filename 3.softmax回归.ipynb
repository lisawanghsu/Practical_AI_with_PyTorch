{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.softmax回归\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "1. 熟悉softmax回归算法和模型构建方法\n",
    "\n",
    "2. 熟练使用Softmax激活函数\n",
    "\n",
    "3. 熟练使用交叉熵（CE）损失函数\n",
    "\n",
    "4. 能够使用torchvision.datasets.MNIST加载MNIST数据集\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 softmax回归\n",
    "\n",
    "逻辑回归主要用于二分类问题，即预测两个类别中的一个。它输出的是事件发生的概率。softmax回归可以看作是逻辑回归在多分类问题上的推广，它将一个样本的特征向量映射为概率分布。\n",
    "\n",
    "Softmax回归用于多分类问题，能够处理两个以上类别的情况。它输出的是每个类别的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 MNIST数据集\n",
    "\n",
    "MNIST数据集起源于美国国家标准与技术研究所（National Institute of Standards and Technology，简称NIST）。该数据集最初由NIST收集整理，由Yann LeCun等人在20世纪90年代创建，目的是通过算法实现对手写数字的识别。MNIST数据集是深度学习和图像识别研究中的一个基准数据集，包含了来自250个不同人的手写数字图片，其中一半是高中生的手写样本，另一半则来自人口普查局的工作人员。它包含了60,000个训练样本和10,000个测试样本，每个样本是一个28x28像素的灰度图像，代表从0到9的手写数字。\n",
    "\n",
    "3.3 MNIST数据集的特点\n",
    "\n",
    "简单性：由于图像都是标准化的尺寸和简单的手写数字，MNIST被认为是图像识别任务的一个相对简单的起点。\n",
    "\n",
    "多样性：尽管每个数字只有10个样本，但手写风格的差异为模型提供了一定的挑战性。\n",
    "\n",
    "广泛性：MNIST被广泛用于机器学习和深度学习算法的测试和训练，尤其是在卷积神经网络(CNN)的入门教程中。\n",
    "\n",
    "基准测试：MNIST常被用作评估新算法性能的基准，因为它的数据量适中，且任务明确。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 基于softmax回归的手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.导入必需的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.数据集的加载和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms.Compose 将transforms.ToTensor和transforms.Normalize等转换步骤组合起来，创建的预处理流程可以一次性应用到图像上。这使得数据预处理步骤的代码更加简洁和易于管理。\n",
    "\n",
    " 在transforms.Normalize((0.5,), (0.5,)) 中：\n",
    "\n",
    "第一个参数 (0.5,) 表示均值，它是一个单元素的元组，意味着所有通道的均值都是 0.5。由于只有一个元素，这通常用于灰度图像。\n",
    "\n",
    "第二个参数 (0.5,) 表示标准差，它也是一个单元素的元组，意味着所有通道的标准差都是 0.5。\n",
    "\n",
    "当你将这个变换应用到图像数据上时，每个通道的每个像素值将被减去均值 0.5 然后除以标准差 0.5。\n",
    "\n",
    "这将导致变换后的像素值范围从原始的 [0, 1] 或 [0, 255] 变为 [-1, 1]。这种标准化有助于模型更快地收敛，因为它确保了不同通道和不同像素值的分布更加一致。\n",
    "\n",
    "实际使用中，均值和标准差通常是根据整个数据集的统计数据计算得到的，以便更好地反映数据的分布特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载训练集和测试集\n",
    "train_dataset = datasets.MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./datasets', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets.MNIST 是 PyTorch 的 torchvision.datasets 模块中的一个类，它用于加载和处理 MNIST 数据集。\n",
    "\n",
    "使用 datasets.MNIST 加载数据集时，你可以指定数据集的根目录、转换（transforms）、下载方式等参数。这个类提供了一个方法 __getitem__ 来访问数据集中的每个样本，以及 __len__ 方法来获取数据集的大小。\n",
    "\n",
    "使用 datasets.MNIST 类加载了训练集和测试集，通过 download=True 参数自动下载数据集（如果本地没有的话）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader的作用是将数据集封装用于后面的训练，我们使用DataLoader来加载训练集和验证集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28  # MNIST图像大小为28x28\n",
    "num_classes = 10   # 总共有10个类别（0到9）\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）构建Softmax回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(input_dim, num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.CrossEntropyLoss()在PyTorch中已经内置了softmax函数。这个损失函数通常用于多分类问题，它结合了softmax层和负对数似然损失（negative log likelihood loss），因此你不需要在损失函数之前手动添加softmax层。\n",
    "nn.CrossEntropyLoss的输入要求是未归一化的分数（即模型的原始输出），它将这些分数通过softmax函数转换为概率分布，然后计算这些概率与目标类别之间的交叉熵损失。\n",
    "\n",
    "Softmax函数是一种常用的激活函数，特别是在处理多分类问题时。它将一个向量或一组实数转换成概率分布，使得每个元素的值都在0到1之间，并且所有元素的和为1。这使得Softmax函数非常适合用作神经网络输出层的激活函数，用于预测多个类别的概率。\n",
    "\n",
    "在多分类问题中，Softmax函数通常与交叉熵损失（Cross-Entropy Loss）一起使用，这有助于优化模型以正确分类数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.60564\n",
      "Epoch [2/10], Loss: 0.38594\n",
      "Epoch [3/10], Loss: 0.35212\n",
      "Epoch [4/10], Loss: 0.33493\n",
      "Epoch [5/10], Loss: 0.32395\n",
      "Epoch [6/10], Loss: 0.31573\n",
      "Epoch [7/10], Loss: 0.31012\n",
      "Epoch [8/10], Loss: 0.30493\n",
      "Epoch [9/10], Loss: 0.30125\n",
      "Epoch [10/10], Loss: 0.29816\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0  # 初始化总损失累加器\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        \n",
    "        outputs = model(data.view(-1, input_dim))\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()  # 累加损失值\n",
    "\n",
    "    # 计算平均损失并打印\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.6%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, labels in test_loader:\n",
    "        \n",
    "        outputs = model(data.view(-1, input_dim))\n",
    "        # print(outputs[0])\n",
    "        _, predicted = torch.max(outputs, 1)  # _ 最大值，predicted 最大值索引\n",
    "        # print(predicted[0], labels[0])\n",
    "        # print(labels.shape, labels.size(0))\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
